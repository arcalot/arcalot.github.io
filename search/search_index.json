{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Arcalot: Another Repository Containing A Lot Of Things The Arcalot community develops tools, plugins, and libraries that you can use either standalone as a library, and/or via a user interface or CLI. You can run the tools locally, remotely, or as part of a bigger system. Arcalot: Helps you create workflows with normalized input and output schemas Provides you with assisted and automated root cause analysis for the workflows you create as well as CI and other log systems Provides stable plugins for several workloads Arcaflow Arcaflow is a workflow engine consisting of three main components: Core engine UI Plugins (including SDKs for Go and Python to write your own plugins) It allows you to click and drag plugins into a workflow for your systems and, if needed, feed the resulting data back into the UI for further analysis. You can also use it just to generate a workflow with parallel and subsequent tasks via the command line. There is a range of supported plugins, written either in Go or Python. Read more Arcalog Arcalog can assist you with or automate your root cause analysis in CI or other log systems either as a standalone tool or by embedding it into your applications. It also provides additional tooling to download jobs from various log systems or add your own log files for analysis. Read more Community You can find our general community health files like our code of conduct and contribution guidelines in the .github repository . If you have any questions or suggestions, please use the issues in the respective repository or contribute to the discussions . If you would like to contribute, check out the issues in the individual repositories and our project boards where we organize our work. If you want to participate in our bi-weekly meeting you can add our recurring meeting to your calendar: Arcalot Project Community meeting (iCal)","title":"Home"},{"location":"#arcalot-another-repository-containing-a-lot-of-things","text":"The Arcalot community develops tools, plugins, and libraries that you can use either standalone as a library, and/or via a user interface or CLI. You can run the tools locally, remotely, or as part of a bigger system. Arcalot: Helps you create workflows with normalized input and output schemas Provides you with assisted and automated root cause analysis for the workflows you create as well as CI and other log systems Provides stable plugins for several workloads","title":"Arcalot: Another Repository Containing A Lot Of Things"},{"location":"#arcaflow","text":"Arcaflow is a workflow engine consisting of three main components: Core engine UI Plugins (including SDKs for Go and Python to write your own plugins) It allows you to click and drag plugins into a workflow for your systems and, if needed, feed the resulting data back into the UI for further analysis. You can also use it just to generate a workflow with parallel and subsequent tasks via the command line. There is a range of supported plugins, written either in Go or Python. Read more","title":"Arcaflow"},{"location":"#arcalog","text":"Arcalog can assist you with or automate your root cause analysis in CI or other log systems either as a standalone tool or by embedding it into your applications. It also provides additional tooling to download jobs from various log systems or add your own log files for analysis. Read more","title":"Arcalog"},{"location":"#community","text":"You can find our general community health files like our code of conduct and contribution guidelines in the .github repository . If you have any questions or suggestions, please use the issues in the respective repository or contribute to the discussions . If you would like to contribute, check out the issues in the individual repositories and our project boards where we organize our work. If you want to participate in our bi-weekly meeting you can add our recurring meeting to your calendar: Arcalot Project Community meeting (iCal)","title":"Community"},{"location":"arcaflow/","text":"Arcaflow: The noble workflow engine Arcaflow is a workflow engine in development which provides the ability to execute workflow steps in sequence, in parallel, repeatedly, etc. The main difference to competitors such as Netflix Conductor is the ability to run ad-hoc workflows without an infrastructure setup required. The engine uses containers to execute plugins and runs them either locally in Docker/Podman or remotely on a Kubernetes cluster. The workflow system is strongly typed and allows for generating JSON schema and OpenAPI documents for all data formats involved. A long-term goal is to provide the ability to package workflows and the engine together into CLI tools , webservices , Kubernetes operators , or integrated in a CI system with no additional coding work involved. The planned user interface will allow for editing workflows and reviewing the executions in detail. Roadmap Our roadmap can be found on GitHub . Use cases Our primary use case at this time is running performance and chaos testing tools on demand . Secondary use cases that we are considering: QE/testing Security testing ETL and other infrastructure migration jobs (once data streaming is available) Architecture flowchart LR subgraph laptop[Your laptop] direction LR ui(UI) engine(Engine) git(Git) ui -- Workflow --> engine ui -- Workflow --> git -- Workflow --> engine engine -- Execution results --> ui end subgraph docker[Docker/Podman<br>on your laptop] direction LR plugins1(Plugin) engine -- Step execution --> plugins1 end engine -- Launch plugin --> docker subgraph k8s[Kubernetes] direction LR plugins2(Plugin) engine -- Step execution --> plugins2 end engine -- Launch plugin --> k8s apis(Other APIs) plugins1 --> apis plugins2 --> apis The Arcaflow architecture consists of the following 3 keys elements: Plugins The Engine The User Interface Schemas A core element of the Arcaflow system is the schema system. Each plugin and the engine itself will provide a machine-readable data structure that describes what inputs are expected and what outputs may be produced. If you are familiar with JSON schema or OpenAPI, this is similar, and Arcaflow can produce those schema documents. However, the Arcaflow system is stricter than those industry standards to optimize for performance and simpler implementation in all supported programming languages. Plugins Plugins provide execution for one or more steps for a workflow. The job of a plugin is to do one job and do it well . They provide a thin layer over third party tools, or an own implementation of features. Their main job is to provide accurate input and output schema information to the engine and transform the data as needed. For example, a plugin may output unformatted text, which a plugin has to parse and build a machine-readable data structure for that information. This reformatting of data allows the engine to pipe data between steps and reliably check the data for faults. The current plan is to provide plugin SDKs for Python, GO, and Rust (in that order). Engine The engine is responsible for the orchestration of the workflow steps. It has several duties: Provide schemas for workflow files, read workflows and construct execution graphs. Type-check the execution graphs to make sure that the data transfers between steps are typesafe. Orchestrate plugin execution with Docker, Podman and Kubernetes. Execute the workflow, following the workflow rules . The engine itself is designed to be run from a command line interface, possibly as a webserver, but is not designed to run in a redundant fashion. Instead of implementing redundancy itself, the engine will receive support to execute workflows in third party systems, such as Kafka. A stretch goal for the engine is to make it fully embeddable, possibly with in-binary workflows and execution images to make them easily to ship in network-restricted environments. User Interface The user interface has two goals: Allow users to edit workflows Inspect workflow results, debugging possible failures Future possible extensions will allow for integrating the user interface into other systems for using the workflow engine as an embedded system.","title":"Arcaflow: The noble workflow engine"},{"location":"arcaflow/#arcaflow-the-noble-workflow-engine","text":"Arcaflow is a workflow engine in development which provides the ability to execute workflow steps in sequence, in parallel, repeatedly, etc. The main difference to competitors such as Netflix Conductor is the ability to run ad-hoc workflows without an infrastructure setup required. The engine uses containers to execute plugins and runs them either locally in Docker/Podman or remotely on a Kubernetes cluster. The workflow system is strongly typed and allows for generating JSON schema and OpenAPI documents for all data formats involved. A long-term goal is to provide the ability to package workflows and the engine together into CLI tools , webservices , Kubernetes operators , or integrated in a CI system with no additional coding work involved. The planned user interface will allow for editing workflows and reviewing the executions in detail.","title":"Arcaflow: The noble workflow engine"},{"location":"arcaflow/#roadmap","text":"Our roadmap can be found on GitHub .","title":"Roadmap"},{"location":"arcaflow/#use-cases","text":"Our primary use case at this time is running performance and chaos testing tools on demand . Secondary use cases that we are considering: QE/testing Security testing ETL and other infrastructure migration jobs (once data streaming is available)","title":"Use cases"},{"location":"arcaflow/#architecture","text":"flowchart LR subgraph laptop[Your laptop] direction LR ui(UI) engine(Engine) git(Git) ui -- Workflow --> engine ui -- Workflow --> git -- Workflow --> engine engine -- Execution results --> ui end subgraph docker[Docker/Podman<br>on your laptop] direction LR plugins1(Plugin) engine -- Step execution --> plugins1 end engine -- Launch plugin --> docker subgraph k8s[Kubernetes] direction LR plugins2(Plugin) engine -- Step execution --> plugins2 end engine -- Launch plugin --> k8s apis(Other APIs) plugins1 --> apis plugins2 --> apis The Arcaflow architecture consists of the following 3 keys elements: Plugins The Engine The User Interface","title":"Architecture"},{"location":"arcaflow/#schemas","text":"A core element of the Arcaflow system is the schema system. Each plugin and the engine itself will provide a machine-readable data structure that describes what inputs are expected and what outputs may be produced. If you are familiar with JSON schema or OpenAPI, this is similar, and Arcaflow can produce those schema documents. However, the Arcaflow system is stricter than those industry standards to optimize for performance and simpler implementation in all supported programming languages.","title":"Schemas"},{"location":"arcaflow/#plugins","text":"Plugins provide execution for one or more steps for a workflow. The job of a plugin is to do one job and do it well . They provide a thin layer over third party tools, or an own implementation of features. Their main job is to provide accurate input and output schema information to the engine and transform the data as needed. For example, a plugin may output unformatted text, which a plugin has to parse and build a machine-readable data structure for that information. This reformatting of data allows the engine to pipe data between steps and reliably check the data for faults. The current plan is to provide plugin SDKs for Python, GO, and Rust (in that order).","title":"Plugins"},{"location":"arcaflow/#engine","text":"The engine is responsible for the orchestration of the workflow steps. It has several duties: Provide schemas for workflow files, read workflows and construct execution graphs. Type-check the execution graphs to make sure that the data transfers between steps are typesafe. Orchestrate plugin execution with Docker, Podman and Kubernetes. Execute the workflow, following the workflow rules . The engine itself is designed to be run from a command line interface, possibly as a webserver, but is not designed to run in a redundant fashion. Instead of implementing redundancy itself, the engine will receive support to execute workflows in third party systems, such as Kafka. A stretch goal for the engine is to make it fully embeddable, possibly with in-binary workflows and execution images to make them easily to ship in network-restricted environments.","title":"Engine"},{"location":"arcaflow/#user-interface","text":"The user interface has two goals: Allow users to edit workflows Inspect workflow results, debugging possible failures Future possible extensions will allow for integrating the user interface into other systems for using the workflow engine as an embedded system.","title":"User Interface"},{"location":"arcaflow/getting-started/","text":"Arcaflow Getting Started Guide Introduction Arcaflow is a modular system that enables engineers to easily build complex parallelized workflows without requiring any pre-installation or deployment of prerequisite software stacks. It has a plugin architecture that is used to build workflows , and execution of the workflows is coordinated by an engine . Workflows are composable and highly portable, capable of orchestrating complex interrelated actions between plugins. This effectively allows for engineering workflow expertise to be packaged up, version controlled, and shared for repeatability across environments and platforms. The engine component is intended to run from your laptop, jump host, or wherever you have network connectivity to the target environment. No installation is required of any Arcaflow components, either on your system or the target environment. The engine processes the workflow definition, invokes the plugins in the target environment, and passes the required data to the plugins as directed by the workflow. The plugins are containers that can speak the engine\u2019s CBOR protocol and that have explicitly defined input and output schemas. Plugins can be run locally via Docker or Podman, or in a remote Kubernetes cluster (and in the future on a remote system via SSH and Docker/Podman). Plugins can also be run alone from the command line, independent of the engine. Workflow Basics Logically, a workflow looks like a path being followed from one plugin to another. Functionally, it works in a star pattern with the engine at the center. So the output of a plugin is passed back to the engine, which then hands off data to the next plugin(s). Logical Flow flowchart TD input(input)-->plugin1[plugin] input-->plugin2[plugin] input-->plugin3[plugin] plugin1-->plugin4[plugin] plugin2-->plugin4 plugin3--->output(output) plugin4-->output Functional Flow flowchart TD input(input)-->engine((engine)) engine-->plugin1[plugin]-->engine engine-->plugin2[plugin]-->engine engine-->plugin3[plugin]-->engine engine-->output(output) Example Workflow Consider below a sample workflow that runs a sysbench CPU load, a PCP metrics collector, an Ansible gather facts metadata collector, and sends the output to an Elasticsearch index. A bonus feature we get from the engine is that it will output the workflow in mermaid format for rendering, so we can see that it looks like this visually (error nodes are removed below for simplicity): flowchart LR subgraph input input.sysbench_threads input.sysbench_events input.elastic_password input.sysbench_runtime input.elastic_index input.pmlogger_interval input.sysbench_cpumaxprime input.elastic_username input.elastic_host end input.sysbench_threads-->steps.sysbench steps.sysbench-->steps.sysbench.outputs.success steps.metadata-->steps.metadata.outputs.success steps.sysbench.outputs.success-->output steps.sysbench.outputs.success-->steps.elasticsearch input.sysbench_events-->steps.sysbench input.elastic_password-->steps.elasticsearch input.sysbench_runtime-->steps.pcp input.sysbench_runtime-->steps.sysbench input.elastic_index-->steps.elasticsearch input.pmlogger_interval-->steps.pcp steps.pcp.outputs.success-->steps.elasticsearch steps.pcp.outputs.success-->output steps.pcp-->steps.pcp.outputs.success steps.metadata.outputs.success-->steps.elasticsearch steps.metadata.outputs.success-->output input.sysbench_cpumaxprime-->steps.sysbench input.elastic_username-->steps.elasticsearch input.elastic_host-->steps.elasticsearch steps.elasticsearch-->steps.elasticsearch.outputs.success This example workflow expects input parameters that are passed down to the plugins, in this case to the PCP , sysbench , and elasticsearch plugins. These parameters determine the details of the actions that those plugins will perform. The metadata plugin used here requires no inputs, as evidenced in the workflow diagram. The success outputs of the PCP, sysbench, and metadata plugins are then directed both to the elasticsearch plugin and to the output of the workflow. Note: The complete example workflow definition is below in the Using and Contributing Level 2 section. Using and Contributing The entry point for working with Arcaflow as a new user is intended to be very low. Pre-composed workflows are things you should be able to download and use with very little in terms of prerequisites, allowing you with little experience to run workflows in the same ways as domain experts. Moving up through layers of sophistication as a user and contributor should be equally straightforward. As you become familiar with running workflows , you may be drawn into tweaking and modifying those workflows to suit your needs, or further into authoring new plugins to expand functionality or even into contributing to our core components . The sections below should give you an idea of how to get started at each level of sophistication. Level 0 Yeoman - Workflow User Welcome to the community! We are more than happy to help you get started. You should get a feel for things by running a workflow or two. Below is an example you can try out. You will need a Golang runtime and Docker to run the containers (Podman can be used with the system service enabled for socket connections, which are required by the engine to communicate with the plugins). The example used here is the same one from the Example Workflow section above. Clone the engine : $ git clone https://github.com/arcalot/arcaflow-engine.git Clone the workflows and set your workflow directory path: $ git clone https://github.com/arcalot/arcaflow-workflows.git $ export WFPATH=$(pwd)/arcaflow-workflows/example-workflow Run the workflow (the containers will run on your local machine via Docker): $ cd arcaflow-engine $ go run cmd/arcaflow/main.go -input ${WFPATH}/input.yaml -config ${WFPATH}/config.yaml -context ${WFPATH} The config.yaml file is set for debug output, so you\u2019ll get a lot returned to the terminal (including the mermaid code mentioned above). This workflow is set to simply return the output of the various plugins, so in the end you\u2019ll get that formatted content dumped to the terminal. Level 1 Page - Advanced Workflow User The next layer of sophistication is modifying the test parameters, which are in the input.yaml file referenced in the above command. For our simple workflow here, you have parameters to adjust how the CPU stress test is performed and to adjust the resolution of PCP\u2019s data collection. You can freely adjust these parameters to your needs without needing to change anything about the workflow itself. This requires some understanding of what the underlying sysbench and PCP tools do, and of course caution should be taken since this test will apply load to the system where it is run. The example input.yaml file looks like this: pmlogger_interval : 1 sysbench_threads : 20 sysbench_events : 0 sysbench_cpumaxprime : 12000 sysbench_runtime : 20 elastic_host : foo elastic_username : foo elastic_password : foo elastic_index : foo Level 2 Squire - Workflow Creator So you\u2019re more adventurous and want to change/author workflows? This is the layer at which we think most of our technical users will spend a majority of their effort. Our goals of packaging, version controlling, and shipping domain expertise are primarily facilitated here. A workflow author dreams up a series of tests with various loops and parallelizations they want to run, determines how to collect and transport data and metadata, and then bundles this all into a workflow that can be shared as easily as the one presented in the examples above. A workflow user just needs the engine and your workflow file. That\u2019s it. The workflow file has an input section where it defines the workflow schema. The workflow author can get creative here in how they want to represent the available parameters to the user, and there are mechanics for self-documentation when good practices are followed with schema metadata. Then there is a steps section, which defines the plugins and their relationships. In the reference examples here, we show some data passing both from the input section to the plugins and between plugins where outputs are passed to the elasticsearch plugin (we have another example with uperf that gets more complicated with kubernetes and data passing, once you\u2019re ready to dig in more). Then finally the output section defines what the workflow will return to the user. The complete workflow definition for the example above looks like this: input : root : RootObject objects : RootObject : id : RootObject properties : pmlogger_interval : display : description : The logger collection interval for PCP pmlogger name : PCP pmlogger collection interval type : type_id : integer sysbench_threads : display : description : The number of threads sysbench will run name : sysbench threads type : type_id : integer sysbench_events : display : description : The number of events sysbench will run name : sysbench events type : type_id : integer sysbench_cpumaxprime : display : description : The upper limit of the number of prime numbers generated name : sysbench cpu max primes type : type_id : integer sysbench_runtime : display : description : The total runtime in seconds for the sysbench tests name : sysbench runtime seconds type : type_id : integer elastic_host : display : description : The host URL for the ElasticSearch service name : elasticsearch host url type : type_id : string elastic_username : display : description : The username for the ElasticSearch service name : elasticsearch username type : type_id : string elastic_password : display : description : The password for the ElasticSearch service name : elasticsearch password type : type_id : string elastic_index : display : description : The index for the ElasticSearch service name : elasticsearch index type : type_id : string steps : pcp : plugin : quay.io/dustinblack/arcaflow-plugin-pcp-test:latest step : start-pcp input : pmlogger_interval : !expr $.input.pmlogger_interval run_duration : !expr $.input.sysbench_runtime sysbench : plugin : quay.io/arcalot/arcaflow-plugin-sysbench:latest step : sysbenchcpu input : operation : cpu threads : !expr $.input.sysbench_threads events : !expr $.input.sysbench_events cpumaxprime : !expr $.input.sysbench_cpumaxprime time : !expr $.input.sysbench_runtime metadata : plugin : quay.io/arcalot/arcaflow-plugin-metadata:latest input : {} elasticsearch : plugin : quay.io/arcalot/arcaflow-plugin-elasticsearch:latest input : url : !expr $.input.elastic_host username : !expr $.input.elastic_username password : !expr $.input.elastic_password index : !expr $.input.elastic_index data : pcp : !expr $.steps.pcp.outputs.success sysbench : !expr $.steps.sysbench.outputs.success metadata : !expr $.steps.metadata.outputs.success output : pcp : !expr $.steps.pcp.outputs.success sysbench : !expr $.steps.sysbench.outputs.success metadata : !expr $.steps.metadata.outputs.success Level 3 Knight - Plugin Author So now you want to build plugins or add features to existing plugins? Great! Welcome to the Arcalot Round Table ! We provide Software Development Kits (SDKs) for Python and Golang to get you started. The main thing you need to understand is that a plugin is expected to define and adhere to its schemas, so the SDK enforces strict typing, which can be a little strange at first for a Python developer. Optimally, a plugin follows the Unix philosophy of do one thing and do it well , and a plugin should not be created in an opinionated way. When creating a plugin, you should consider how you will expose all parameters and configuration values via a single schema, and collect all output similarly in a single schema. The plugins provide the API endpoints for functions or actions, and workflows glue together those endpoints to make use of the data in an opinionated way. Sometimes creating a schema for an existing tool is relatively easy , sometimes it is pretty involved . You can start digging into the technical details more in the Concepts and Plugins sections of the documentation. Level 4 Liege - Core Components Contributor If you have a grasp on everything above and still want to dig in deeper, then welcome to core components development! Here you can participate in the development of the workflow engine and its related libraries, the SDKs, the build system , and more. Community If you\u2019re looking for a place to jump in and help, have a look at our Project boards and our Discussions page. We also manage our charter, code of conduct, and licensing, as well as significant project decisions, via our Arcalot Round Table project. We can also always use help with our documentation efforts.","title":"Getting Started"},{"location":"arcaflow/getting-started/#arcaflow-getting-started-guide","text":"","title":"Arcaflow Getting Started Guide"},{"location":"arcaflow/getting-started/#introduction","text":"Arcaflow is a modular system that enables engineers to easily build complex parallelized workflows without requiring any pre-installation or deployment of prerequisite software stacks. It has a plugin architecture that is used to build workflows , and execution of the workflows is coordinated by an engine . Workflows are composable and highly portable, capable of orchestrating complex interrelated actions between plugins. This effectively allows for engineering workflow expertise to be packaged up, version controlled, and shared for repeatability across environments and platforms. The engine component is intended to run from your laptop, jump host, or wherever you have network connectivity to the target environment. No installation is required of any Arcaflow components, either on your system or the target environment. The engine processes the workflow definition, invokes the plugins in the target environment, and passes the required data to the plugins as directed by the workflow. The plugins are containers that can speak the engine\u2019s CBOR protocol and that have explicitly defined input and output schemas. Plugins can be run locally via Docker or Podman, or in a remote Kubernetes cluster (and in the future on a remote system via SSH and Docker/Podman). Plugins can also be run alone from the command line, independent of the engine.","title":"Introduction"},{"location":"arcaflow/getting-started/#workflow-basics","text":"Logically, a workflow looks like a path being followed from one plugin to another. Functionally, it works in a star pattern with the engine at the center. So the output of a plugin is passed back to the engine, which then hands off data to the next plugin(s).","title":"Workflow Basics"},{"location":"arcaflow/getting-started/#logical-flow","text":"flowchart TD input(input)-->plugin1[plugin] input-->plugin2[plugin] input-->plugin3[plugin] plugin1-->plugin4[plugin] plugin2-->plugin4 plugin3--->output(output) plugin4-->output","title":"Logical Flow"},{"location":"arcaflow/getting-started/#functional-flow","text":"flowchart TD input(input)-->engine((engine)) engine-->plugin1[plugin]-->engine engine-->plugin2[plugin]-->engine engine-->plugin3[plugin]-->engine engine-->output(output)","title":"Functional Flow"},{"location":"arcaflow/getting-started/#example-workflow","text":"Consider below a sample workflow that runs a sysbench CPU load, a PCP metrics collector, an Ansible gather facts metadata collector, and sends the output to an Elasticsearch index. A bonus feature we get from the engine is that it will output the workflow in mermaid format for rendering, so we can see that it looks like this visually (error nodes are removed below for simplicity): flowchart LR subgraph input input.sysbench_threads input.sysbench_events input.elastic_password input.sysbench_runtime input.elastic_index input.pmlogger_interval input.sysbench_cpumaxprime input.elastic_username input.elastic_host end input.sysbench_threads-->steps.sysbench steps.sysbench-->steps.sysbench.outputs.success steps.metadata-->steps.metadata.outputs.success steps.sysbench.outputs.success-->output steps.sysbench.outputs.success-->steps.elasticsearch input.sysbench_events-->steps.sysbench input.elastic_password-->steps.elasticsearch input.sysbench_runtime-->steps.pcp input.sysbench_runtime-->steps.sysbench input.elastic_index-->steps.elasticsearch input.pmlogger_interval-->steps.pcp steps.pcp.outputs.success-->steps.elasticsearch steps.pcp.outputs.success-->output steps.pcp-->steps.pcp.outputs.success steps.metadata.outputs.success-->steps.elasticsearch steps.metadata.outputs.success-->output input.sysbench_cpumaxprime-->steps.sysbench input.elastic_username-->steps.elasticsearch input.elastic_host-->steps.elasticsearch steps.elasticsearch-->steps.elasticsearch.outputs.success This example workflow expects input parameters that are passed down to the plugins, in this case to the PCP , sysbench , and elasticsearch plugins. These parameters determine the details of the actions that those plugins will perform. The metadata plugin used here requires no inputs, as evidenced in the workflow diagram. The success outputs of the PCP, sysbench, and metadata plugins are then directed both to the elasticsearch plugin and to the output of the workflow. Note: The complete example workflow definition is below in the Using and Contributing Level 2 section.","title":"Example Workflow"},{"location":"arcaflow/getting-started/#using-and-contributing","text":"The entry point for working with Arcaflow as a new user is intended to be very low. Pre-composed workflows are things you should be able to download and use with very little in terms of prerequisites, allowing you with little experience to run workflows in the same ways as domain experts. Moving up through layers of sophistication as a user and contributor should be equally straightforward. As you become familiar with running workflows , you may be drawn into tweaking and modifying those workflows to suit your needs, or further into authoring new plugins to expand functionality or even into contributing to our core components . The sections below should give you an idea of how to get started at each level of sophistication.","title":"Using and Contributing"},{"location":"arcaflow/getting-started/#level-0","text":"Yeoman - Workflow User Welcome to the community! We are more than happy to help you get started. You should get a feel for things by running a workflow or two. Below is an example you can try out. You will need a Golang runtime and Docker to run the containers (Podman can be used with the system service enabled for socket connections, which are required by the engine to communicate with the plugins). The example used here is the same one from the Example Workflow section above. Clone the engine : $ git clone https://github.com/arcalot/arcaflow-engine.git Clone the workflows and set your workflow directory path: $ git clone https://github.com/arcalot/arcaflow-workflows.git $ export WFPATH=$(pwd)/arcaflow-workflows/example-workflow Run the workflow (the containers will run on your local machine via Docker): $ cd arcaflow-engine $ go run cmd/arcaflow/main.go -input ${WFPATH}/input.yaml -config ${WFPATH}/config.yaml -context ${WFPATH} The config.yaml file is set for debug output, so you\u2019ll get a lot returned to the terminal (including the mermaid code mentioned above). This workflow is set to simply return the output of the various plugins, so in the end you\u2019ll get that formatted content dumped to the terminal.","title":"Level 0"},{"location":"arcaflow/getting-started/#level-1","text":"Page - Advanced Workflow User The next layer of sophistication is modifying the test parameters, which are in the input.yaml file referenced in the above command. For our simple workflow here, you have parameters to adjust how the CPU stress test is performed and to adjust the resolution of PCP\u2019s data collection. You can freely adjust these parameters to your needs without needing to change anything about the workflow itself. This requires some understanding of what the underlying sysbench and PCP tools do, and of course caution should be taken since this test will apply load to the system where it is run. The example input.yaml file looks like this: pmlogger_interval : 1 sysbench_threads : 20 sysbench_events : 0 sysbench_cpumaxprime : 12000 sysbench_runtime : 20 elastic_host : foo elastic_username : foo elastic_password : foo elastic_index : foo","title":"Level 1"},{"location":"arcaflow/getting-started/#level-2","text":"Squire - Workflow Creator So you\u2019re more adventurous and want to change/author workflows? This is the layer at which we think most of our technical users will spend a majority of their effort. Our goals of packaging, version controlling, and shipping domain expertise are primarily facilitated here. A workflow author dreams up a series of tests with various loops and parallelizations they want to run, determines how to collect and transport data and metadata, and then bundles this all into a workflow that can be shared as easily as the one presented in the examples above. A workflow user just needs the engine and your workflow file. That\u2019s it. The workflow file has an input section where it defines the workflow schema. The workflow author can get creative here in how they want to represent the available parameters to the user, and there are mechanics for self-documentation when good practices are followed with schema metadata. Then there is a steps section, which defines the plugins and their relationships. In the reference examples here, we show some data passing both from the input section to the plugins and between plugins where outputs are passed to the elasticsearch plugin (we have another example with uperf that gets more complicated with kubernetes and data passing, once you\u2019re ready to dig in more). Then finally the output section defines what the workflow will return to the user. The complete workflow definition for the example above looks like this: input : root : RootObject objects : RootObject : id : RootObject properties : pmlogger_interval : display : description : The logger collection interval for PCP pmlogger name : PCP pmlogger collection interval type : type_id : integer sysbench_threads : display : description : The number of threads sysbench will run name : sysbench threads type : type_id : integer sysbench_events : display : description : The number of events sysbench will run name : sysbench events type : type_id : integer sysbench_cpumaxprime : display : description : The upper limit of the number of prime numbers generated name : sysbench cpu max primes type : type_id : integer sysbench_runtime : display : description : The total runtime in seconds for the sysbench tests name : sysbench runtime seconds type : type_id : integer elastic_host : display : description : The host URL for the ElasticSearch service name : elasticsearch host url type : type_id : string elastic_username : display : description : The username for the ElasticSearch service name : elasticsearch username type : type_id : string elastic_password : display : description : The password for the ElasticSearch service name : elasticsearch password type : type_id : string elastic_index : display : description : The index for the ElasticSearch service name : elasticsearch index type : type_id : string steps : pcp : plugin : quay.io/dustinblack/arcaflow-plugin-pcp-test:latest step : start-pcp input : pmlogger_interval : !expr $.input.pmlogger_interval run_duration : !expr $.input.sysbench_runtime sysbench : plugin : quay.io/arcalot/arcaflow-plugin-sysbench:latest step : sysbenchcpu input : operation : cpu threads : !expr $.input.sysbench_threads events : !expr $.input.sysbench_events cpumaxprime : !expr $.input.sysbench_cpumaxprime time : !expr $.input.sysbench_runtime metadata : plugin : quay.io/arcalot/arcaflow-plugin-metadata:latest input : {} elasticsearch : plugin : quay.io/arcalot/arcaflow-plugin-elasticsearch:latest input : url : !expr $.input.elastic_host username : !expr $.input.elastic_username password : !expr $.input.elastic_password index : !expr $.input.elastic_index data : pcp : !expr $.steps.pcp.outputs.success sysbench : !expr $.steps.sysbench.outputs.success metadata : !expr $.steps.metadata.outputs.success output : pcp : !expr $.steps.pcp.outputs.success sysbench : !expr $.steps.sysbench.outputs.success metadata : !expr $.steps.metadata.outputs.success","title":"Level 2"},{"location":"arcaflow/getting-started/#level-3","text":"Knight - Plugin Author So now you want to build plugins or add features to existing plugins? Great! Welcome to the Arcalot Round Table ! We provide Software Development Kits (SDKs) for Python and Golang to get you started. The main thing you need to understand is that a plugin is expected to define and adhere to its schemas, so the SDK enforces strict typing, which can be a little strange at first for a Python developer. Optimally, a plugin follows the Unix philosophy of do one thing and do it well , and a plugin should not be created in an opinionated way. When creating a plugin, you should consider how you will expose all parameters and configuration values via a single schema, and collect all output similarly in a single schema. The plugins provide the API endpoints for functions or actions, and workflows glue together those endpoints to make use of the data in an opinionated way. Sometimes creating a schema for an existing tool is relatively easy , sometimes it is pretty involved . You can start digging into the technical details more in the Concepts and Plugins sections of the documentation.","title":"Level 3"},{"location":"arcaflow/getting-started/#level-4","text":"Liege - Core Components Contributor If you have a grasp on everything above and still want to dig in deeper, then welcome to core components development! Here you can participate in the development of the workflow engine and its related libraries, the SDKs, the build system , and more.","title":"Level 4"},{"location":"arcaflow/getting-started/#community","text":"If you\u2019re looking for a place to jump in and help, have a look at our Project boards and our Discussions page. We also manage our charter, code of conduct, and licensing, as well as significant project decisions, via our Arcalot Round Table project. We can also always use help with our documentation efforts.","title":"Community"},{"location":"arcaflow/concepts/plugin-protocol/","text":"Plugin protocol specification Work in Progress This document is work in progress and may change until the final release! Arcaflow runs plugins locally in a container using Docker or Podman, or remotely in Kubernetes. Each plugin must be containerized and communicates with the engine over standard input/output. This document outlines the protocol the engine and the plugins use to communicate. Hint You do not need this page if you only intend to implement a plugin with the SDK! Execution model A single plugin execution is intended to run a single task and not more. This simplifies the code since there is no need to try and clean up after each task. Each plugin is executed in a container and must communicate with the engine over standard input/output. Furthermore, the plugin must add a handler for SIGTERM and properly clean up if there are services running in the background. Each plugin is executed at the start of the workflow, or workflow block, and is terminated only at the end of the current workflow or workflow block. The plugin can safely rely on being able to start a service in the background and then keeping it running until the SIGTERM comes to shut down the container. However, the plugin must, under no circumstances, start doing work until the engine sends the command to do so. This includes starting any services inside the container or outside. This restriction is necessary to be able to launch the plugin with minimal resource consumption locally on the engine host to fetch the schema. The plugin execution is divided into three major steps. When the plugin is started, it must output the current plugin protocol version and its schema to the standard output. The engine will read this output from the container logs. When it is time to start the work, the engine will send the desired step ID with its input parameters over the standard input. The plugin acknowledges this and starts to work. When the work is complete, the plugin must automatically output the results to the standard output. When a shutdown is desired, the engine will send a SIGTERM to the plugin. The plugin has up to 30 seconds to shut down. The SIGTERM may come at any time, even while the work is still running, and the plugin must appropriately shut down. If the work is not complete, the plugin may attempt to output an error output data to the standard out, but must not do so. If the plugin fails to stop by itself within 30 seconds, the plugin container is forcefully stopped. Protocol As a data transport protocol, we use CBOR messages RFC 8949 back to back due to their self-delimiting nature. This section provides the entire protocol as JSON schema below. Step 0: The \u201cstart output\u201d message Because Kubernetes has no clean way of capturing an output right at the start, the initial step of the plugin execution involves the engine sending an empty CBOR message ( None or Nil ) to the plugin. This indicates, that the plugin may start its output now. Step 1: Hello message The \u201cHello\u201d message is a way for the plugin to introduce itself and present its steps and schema. Transcribed to JSON, a message of this kind would look as follows: { \"version\" : 1 , \"steps\" : { \"step-id-1\" : { \"name\" : \"Step 1\" , \"description\" : \"This is the first step\" , \"input\" : { \"schema\" : { // Input schema } }, \"outputs\" : { \"output-id-1\" : { \"name\" : \"Name for this output kind\" , \"description\" : \"Description for this output\" , \"schema\" : { // Output schema } } } } } } The schemas must describe the data structure the plugin expects. For a simple hello world input would look as follows: { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" } } } The entire \u201chello\u201d message can be described by the following JSON schema: { \"$id\" : \"arcaflow-plugin-v1-hello-message\" , \"$schema\" : \"https://json-schema.org/draft/2020-12/schema\" , \"title\" : \"Hello message\" , \"description\" : \"Initial 'Hello' message from plugin, describing the protocol version and schema of the plugin.\" , \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"version\" : { \"description\" : \"Arcaflow plugin protocol version\" , \"type\" : \"integer\" , \"minimum\" : 1 }, \"steps\" : { \"title\" : \"Steps\" , \"description\" : \"Steps offered by the plugin\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"description\" : \"Name of the step\" , \"type\" : \"string\" }, \"description\" : { \"title\" : \"Description\" , \"description\" : \"Detailed description of this step.\" , \"type\" : \"string\" }, \"input\" : { \"$ref\" : \"#/$defs/object\" } }, \"required\" : [ \"input\" , \"outputs\" ], \"additionalProperties\" : false } } }, \"required\" : [ \"version\" , \"steps\" ], \"$defs\" : { \"type\" : { \"oneOf\" : [ { \"ref\" : \"#/$defs/enum\" }, { \"ref\" : \"#/$defs/object\" }, { \"ref\" : \"#/$defs/string\" }, { \"ref\" : \"#/$defs/pattern\" }, { \"ref\" : \"#/$defs/boolean\" }, { \"ref\" : \"#/$defs/float\" }, { \"ref\" : \"#/$defs/integer\" }, { \"ref\" : \"#/$defs/list\" }, { \"ref\" : \"#/$defs/map\" }, { \"ref\" : \"#/$defs/oneof\" } ] }, \"mapKeys\" : { \"type\" : { \"oneOf\" : [ { \"ref\" : \"#/$defs/enum\" }, { \"ref\" : \"#/$defs/string\" }, { \"ref\" : \"#/$defs/integer\" } ] } }, \"enum\" : { \"title\" : \"Enum\" , \"description\" : \"Enumeration of items\" , \"additionalProperties\" : false , \"required\" : [ \"type\" , \"values\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"enum\" }, \"values\" : { \"type\" : \"array\" , \"items\" : { \"oneOf\" : [ { \"ref\" : \"#/$defs/string\" }, { \"ref\" : \"#/$defs/integer\" } ] } } } }, \"boolean\" : { \"title\" : \"Boolean\" , \"description\" : \"True or false.\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"boolean\" } }, \"additionalProperties\" : false }, \"string\" : { \"title\" : \"String\" , \"description\" : \"A string of characters.\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"string\" }, \"min_length\" : { \"type\" : \"integer\" , \"minimum\" : 0 }, \"max_length\" : { \"type\" : \"integer\" , \"minimum\" : 0 }, \"pattern\" : { \"type\" : \"string\" , \"format\" : \"regex\" } }, \"additionalProperties\" : false }, \"pattern\" : { \"title\" : \"Pattern\" , \"description\" : \"A regular expression.\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"pattern\" } }, \"additionalProperties\" : false }, \"integer\" : { \"title\" : \"Integer\" , \"description\" : \"64-bit integers\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"integer\" }, \"min\" : { \"type\" : \"integer\" , \"minimum\" : 0 }, \"max\" : { \"type\" : \"integer\" , \"minimum\" : 0 } }, \"additionalProperties\" : false }, \"float\" : { \"title\" : \"Float\" , \"description\" : \"64-bit floating point numbers.\" , \"type\" : \"object\" , \"required\" : { \"type\" }, \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"float\" }, \"min\" : { \"type\" : \"number\" , \"minimum\" : 0 }, \"max\" : { \"type\" : \"number\" , \"minimum\" : 0 } }, \"additionalProperties\" : false }, \"list\" : { \"title\" : \"List\" , \"description\" : \"A list of predefined types.\" , \"type\" : \"object\" , \"required\" : [ \"items\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"list\" }, \"min\" : { \"title\" : \"Minimum items\" , \"description\" : \"The minimum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"max\" : { \"title\" : \"Maximum items\" , \"description\" : \"The maximum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"items\" : { \"title\" : \"Items\" , \"description\" : \"Type definition for items in the list.\" , \"$ref\" : \"#/$defs/type\" } }, \"additionalProperties\" : false }, \"map\" : { \"title\" : \"Map\" , \"description\" : \"A key-value map with defined types.\" , \"type\" : \"object\" , \"required\" : [ \"keys\" , \"values\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"map\" }, \"min\" : { \"title\" : \"Minimum items\" , \"description\" : \"The minimum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"max\" : { \"title\" : \"Maximum items\" , \"description\" : \"The maximum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"keys\" : { \"title\" : \"Keys\" , \"description\" : \"Type definition for keys in the map.\" , \"$ref\" : \"#/$defs/mapKeys\" }, \"values\" : { \"title\" : \"Values\" , \"description\" : \"Type definition for values in the map.\" , \"$ref\" : \"#/$defs/type\" } }, \"additionalProperties\" : false }, \"oneof\" : { \"oneOf\" : [ { \"title\" : \"One Of\" , \"description\" : \"Multiple possible types. The discriminator field is used to determine what type is found.\" , \"type\" : \"object\" , \"required\" : [ \"discriminator_field_name\" , \"discriminator_field_schema\" , \"one_of\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"oneof\" }, \"discriminator_field_name\" : { \"title\" : \"Discriminator field name\" , \"description\" : \"Name for the field that distinguishes between the possible types.\" , \"type\" : \"string\" , \"minimum_length\" : 1 }, \"discriminator_field_schema\" : { \"title\" : \"Discriminator field schema\" , \"description\" : \"Schema for the field that distinguishes between the possible types.\" , \"$ref\" : \"#/$defs/integer\" }, \"one_of\" : { \"title\" : \"One of\" , \"description\" : \"Possible variations.\" , \"type\" : \"object\" , \"propertyNames\" : { \"pattern\" : \"^[0-9]+$\" }, \"additionalProperties\" : { \"$ref\" : \"#/$defs/object\" } } }, \"additionalProperties\" : false }, { \"title\" : \"One Of\" , \"description\" : \"Multiple possible types. The discriminator field is used to determine what type is found.\" , \"type\" : \"object\" , \"required\" : [ \"discriminator_field_name\" , \"discriminator_field_schema\" , \"one_of\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"oneof\" }, \"discriminator_field_name\" : { \"title\" : \"Discriminator field name\" , \"description\" : \"Name for the field that distinguishes between the possible types.\" , \"type\" : \"string\" , \"minimum_length\" : 1 }, \"discriminator_field_schema\" : { \"title\" : \"Discriminator field schema\" , \"description\" : \"Schema for the field that distinguishes between the possible types.\" , \"$ref\" : \"#/$defs/string\" }, \"one_of\" : { \"title\" : \"One of\" , \"description\" : \"Possible variations.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/$defs/object\" } } }, \"additionalProperties\" : false } ] }, \"object\" : { \"title\" : \"Object\" , \"description\" : \"Object type consisting of predefined fields.\" , \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"object\" }, \"properties\" : { \"title\" : \"Properties\" , \"description\" : \"Properties of this object.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"title\" : \"Field\" , \"description\" : \"A property definition in the object.\" , \"type\" : \"object\" , \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"description\" : \"Human-readable name of the property.\" , \"type\" : \"string\" }, \"description\" : { \"title\" : \"Description\" , \"description\" : \"Human-readable description of the property.\" , \"type\" : \"string\" }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Set this field to required.\" , \"type\" : \"boolean\" , \"default\" : false }, \"required_if\" : { \"title\" : \"Required if\" , \"description\" : \"This property is required if any of the listed properties in this field are set.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"required_if_not\" : { \"title\" : \"Required if not\" , \"description\" : \"This property is not required if any of the listed properties in this field are set.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"conflicts\" : { \"title\" : \"Conflicts\" , \"description\" : \"This property must not be set if any of the listed properties in this field are set.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"type\" : { \"$ref\" : \"#/$defs/type\" } }, \"required\" : [ \"type\" ], \"additionalProperties\" : false } } }, \"required\" : [ \"type\" , \"properties\" ] } } } Step 2: Start work message The \u201cstart work\u201d message has the following parameters in CBOR: { \"id\" : \"id-of-the-step-to-execute\" , \"config\" : { // Input parameters according to schema here } } The plugin must respond with a CBOR message of the following format: { \"status\" : \"started\" } Step 3/a: Crash If the plugin execution ended unexpectedly, the plugin should crash and output a reasonable error message to the standard error. The plugin must exit with a non-zero exit status to notify the engine that the execution failed. Step 3/b: Output When the plugin has executed successfully, it must emit a CBOR message to the standard output: { \"output_id\" : \"id-of-the-declared-output\" , \"output_data\" : { // Result data of the plugin }, \"debug_logs\" : \"Unstructured logs here for debugging as a string.\" }","title":"The plugin protocol"},{"location":"arcaflow/concepts/plugin-protocol/#plugin-protocol-specification","text":"Work in Progress This document is work in progress and may change until the final release! Arcaflow runs plugins locally in a container using Docker or Podman, or remotely in Kubernetes. Each plugin must be containerized and communicates with the engine over standard input/output. This document outlines the protocol the engine and the plugins use to communicate. Hint You do not need this page if you only intend to implement a plugin with the SDK!","title":"Plugin protocol specification"},{"location":"arcaflow/concepts/plugin-protocol/#execution-model","text":"A single plugin execution is intended to run a single task and not more. This simplifies the code since there is no need to try and clean up after each task. Each plugin is executed in a container and must communicate with the engine over standard input/output. Furthermore, the plugin must add a handler for SIGTERM and properly clean up if there are services running in the background. Each plugin is executed at the start of the workflow, or workflow block, and is terminated only at the end of the current workflow or workflow block. The plugin can safely rely on being able to start a service in the background and then keeping it running until the SIGTERM comes to shut down the container. However, the plugin must, under no circumstances, start doing work until the engine sends the command to do so. This includes starting any services inside the container or outside. This restriction is necessary to be able to launch the plugin with minimal resource consumption locally on the engine host to fetch the schema. The plugin execution is divided into three major steps. When the plugin is started, it must output the current plugin protocol version and its schema to the standard output. The engine will read this output from the container logs. When it is time to start the work, the engine will send the desired step ID with its input parameters over the standard input. The plugin acknowledges this and starts to work. When the work is complete, the plugin must automatically output the results to the standard output. When a shutdown is desired, the engine will send a SIGTERM to the plugin. The plugin has up to 30 seconds to shut down. The SIGTERM may come at any time, even while the work is still running, and the plugin must appropriately shut down. If the work is not complete, the plugin may attempt to output an error output data to the standard out, but must not do so. If the plugin fails to stop by itself within 30 seconds, the plugin container is forcefully stopped.","title":"Execution model"},{"location":"arcaflow/concepts/plugin-protocol/#protocol","text":"As a data transport protocol, we use CBOR messages RFC 8949 back to back due to their self-delimiting nature. This section provides the entire protocol as JSON schema below.","title":"Protocol"},{"location":"arcaflow/concepts/plugin-protocol/#step-0-the-start-output-message","text":"Because Kubernetes has no clean way of capturing an output right at the start, the initial step of the plugin execution involves the engine sending an empty CBOR message ( None or Nil ) to the plugin. This indicates, that the plugin may start its output now.","title":"Step 0: The \"start output\" message"},{"location":"arcaflow/concepts/plugin-protocol/#step-1-hello-message","text":"The \u201cHello\u201d message is a way for the plugin to introduce itself and present its steps and schema. Transcribed to JSON, a message of this kind would look as follows: { \"version\" : 1 , \"steps\" : { \"step-id-1\" : { \"name\" : \"Step 1\" , \"description\" : \"This is the first step\" , \"input\" : { \"schema\" : { // Input schema } }, \"outputs\" : { \"output-id-1\" : { \"name\" : \"Name for this output kind\" , \"description\" : \"Description for this output\" , \"schema\" : { // Output schema } } } } } } The schemas must describe the data structure the plugin expects. For a simple hello world input would look as follows: { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"type\" : \"string\" } } } The entire \u201chello\u201d message can be described by the following JSON schema: { \"$id\" : \"arcaflow-plugin-v1-hello-message\" , \"$schema\" : \"https://json-schema.org/draft/2020-12/schema\" , \"title\" : \"Hello message\" , \"description\" : \"Initial 'Hello' message from plugin, describing the protocol version and schema of the plugin.\" , \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"version\" : { \"description\" : \"Arcaflow plugin protocol version\" , \"type\" : \"integer\" , \"minimum\" : 1 }, \"steps\" : { \"title\" : \"Steps\" , \"description\" : \"Steps offered by the plugin\" , \"type\" : \"object\" , \"additionalProperties\" : { \"type\" : \"object\" , \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"description\" : \"Name of the step\" , \"type\" : \"string\" }, \"description\" : { \"title\" : \"Description\" , \"description\" : \"Detailed description of this step.\" , \"type\" : \"string\" }, \"input\" : { \"$ref\" : \"#/$defs/object\" } }, \"required\" : [ \"input\" , \"outputs\" ], \"additionalProperties\" : false } } }, \"required\" : [ \"version\" , \"steps\" ], \"$defs\" : { \"type\" : { \"oneOf\" : [ { \"ref\" : \"#/$defs/enum\" }, { \"ref\" : \"#/$defs/object\" }, { \"ref\" : \"#/$defs/string\" }, { \"ref\" : \"#/$defs/pattern\" }, { \"ref\" : \"#/$defs/boolean\" }, { \"ref\" : \"#/$defs/float\" }, { \"ref\" : \"#/$defs/integer\" }, { \"ref\" : \"#/$defs/list\" }, { \"ref\" : \"#/$defs/map\" }, { \"ref\" : \"#/$defs/oneof\" } ] }, \"mapKeys\" : { \"type\" : { \"oneOf\" : [ { \"ref\" : \"#/$defs/enum\" }, { \"ref\" : \"#/$defs/string\" }, { \"ref\" : \"#/$defs/integer\" } ] } }, \"enum\" : { \"title\" : \"Enum\" , \"description\" : \"Enumeration of items\" , \"additionalProperties\" : false , \"required\" : [ \"type\" , \"values\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"enum\" }, \"values\" : { \"type\" : \"array\" , \"items\" : { \"oneOf\" : [ { \"ref\" : \"#/$defs/string\" }, { \"ref\" : \"#/$defs/integer\" } ] } } } }, \"boolean\" : { \"title\" : \"Boolean\" , \"description\" : \"True or false.\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"boolean\" } }, \"additionalProperties\" : false }, \"string\" : { \"title\" : \"String\" , \"description\" : \"A string of characters.\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"string\" }, \"min_length\" : { \"type\" : \"integer\" , \"minimum\" : 0 }, \"max_length\" : { \"type\" : \"integer\" , \"minimum\" : 0 }, \"pattern\" : { \"type\" : \"string\" , \"format\" : \"regex\" } }, \"additionalProperties\" : false }, \"pattern\" : { \"title\" : \"Pattern\" , \"description\" : \"A regular expression.\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"pattern\" } }, \"additionalProperties\" : false }, \"integer\" : { \"title\" : \"Integer\" , \"description\" : \"64-bit integers\" , \"type\" : \"object\" , \"required\" : [ \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"integer\" }, \"min\" : { \"type\" : \"integer\" , \"minimum\" : 0 }, \"max\" : { \"type\" : \"integer\" , \"minimum\" : 0 } }, \"additionalProperties\" : false }, \"float\" : { \"title\" : \"Float\" , \"description\" : \"64-bit floating point numbers.\" , \"type\" : \"object\" , \"required\" : { \"type\" }, \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"float\" }, \"min\" : { \"type\" : \"number\" , \"minimum\" : 0 }, \"max\" : { \"type\" : \"number\" , \"minimum\" : 0 } }, \"additionalProperties\" : false }, \"list\" : { \"title\" : \"List\" , \"description\" : \"A list of predefined types.\" , \"type\" : \"object\" , \"required\" : [ \"items\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"list\" }, \"min\" : { \"title\" : \"Minimum items\" , \"description\" : \"The minimum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"max\" : { \"title\" : \"Maximum items\" , \"description\" : \"The maximum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"items\" : { \"title\" : \"Items\" , \"description\" : \"Type definition for items in the list.\" , \"$ref\" : \"#/$defs/type\" } }, \"additionalProperties\" : false }, \"map\" : { \"title\" : \"Map\" , \"description\" : \"A key-value map with defined types.\" , \"type\" : \"object\" , \"required\" : [ \"keys\" , \"values\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"map\" }, \"min\" : { \"title\" : \"Minimum items\" , \"description\" : \"The minimum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"max\" : { \"title\" : \"Maximum items\" , \"description\" : \"The maximum number of items.\" , \"type\" : \"number\" , \"minimum\" : 0 }, \"keys\" : { \"title\" : \"Keys\" , \"description\" : \"Type definition for keys in the map.\" , \"$ref\" : \"#/$defs/mapKeys\" }, \"values\" : { \"title\" : \"Values\" , \"description\" : \"Type definition for values in the map.\" , \"$ref\" : \"#/$defs/type\" } }, \"additionalProperties\" : false }, \"oneof\" : { \"oneOf\" : [ { \"title\" : \"One Of\" , \"description\" : \"Multiple possible types. The discriminator field is used to determine what type is found.\" , \"type\" : \"object\" , \"required\" : [ \"discriminator_field_name\" , \"discriminator_field_schema\" , \"one_of\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"oneof\" }, \"discriminator_field_name\" : { \"title\" : \"Discriminator field name\" , \"description\" : \"Name for the field that distinguishes between the possible types.\" , \"type\" : \"string\" , \"minimum_length\" : 1 }, \"discriminator_field_schema\" : { \"title\" : \"Discriminator field schema\" , \"description\" : \"Schema for the field that distinguishes between the possible types.\" , \"$ref\" : \"#/$defs/integer\" }, \"one_of\" : { \"title\" : \"One of\" , \"description\" : \"Possible variations.\" , \"type\" : \"object\" , \"propertyNames\" : { \"pattern\" : \"^[0-9]+$\" }, \"additionalProperties\" : { \"$ref\" : \"#/$defs/object\" } } }, \"additionalProperties\" : false }, { \"title\" : \"One Of\" , \"description\" : \"Multiple possible types. The discriminator field is used to determine what type is found.\" , \"type\" : \"object\" , \"required\" : [ \"discriminator_field_name\" , \"discriminator_field_schema\" , \"one_of\" , \"type\" ], \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"oneof\" }, \"discriminator_field_name\" : { \"title\" : \"Discriminator field name\" , \"description\" : \"Name for the field that distinguishes between the possible types.\" , \"type\" : \"string\" , \"minimum_length\" : 1 }, \"discriminator_field_schema\" : { \"title\" : \"Discriminator field schema\" , \"description\" : \"Schema for the field that distinguishes between the possible types.\" , \"$ref\" : \"#/$defs/string\" }, \"one_of\" : { \"title\" : \"One of\" , \"description\" : \"Possible variations.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"$ref\" : \"#/$defs/object\" } } }, \"additionalProperties\" : false } ] }, \"object\" : { \"title\" : \"Object\" , \"description\" : \"Object type consisting of predefined fields.\" , \"type\" : \"object\" , \"additionalProperties\" : false , \"properties\" : { \"type\" : { \"type\" : \"string\" , \"const\" : \"object\" }, \"properties\" : { \"title\" : \"Properties\" , \"description\" : \"Properties of this object.\" , \"type\" : \"object\" , \"additionalProperties\" : { \"title\" : \"Field\" , \"description\" : \"A property definition in the object.\" , \"type\" : \"object\" , \"properties\" : { \"name\" : { \"title\" : \"Name\" , \"description\" : \"Human-readable name of the property.\" , \"type\" : \"string\" }, \"description\" : { \"title\" : \"Description\" , \"description\" : \"Human-readable description of the property.\" , \"type\" : \"string\" }, \"required\" : { \"title\" : \"Required\" , \"description\" : \"Set this field to required.\" , \"type\" : \"boolean\" , \"default\" : false }, \"required_if\" : { \"title\" : \"Required if\" , \"description\" : \"This property is required if any of the listed properties in this field are set.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"required_if_not\" : { \"title\" : \"Required if not\" , \"description\" : \"This property is not required if any of the listed properties in this field are set.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"conflicts\" : { \"title\" : \"Conflicts\" , \"description\" : \"This property must not be set if any of the listed properties in this field are set.\" , \"type\" : \"array\" , \"items\" : { \"type\" : \"string\" } }, \"type\" : { \"$ref\" : \"#/$defs/type\" } }, \"required\" : [ \"type\" ], \"additionalProperties\" : false } } }, \"required\" : [ \"type\" , \"properties\" ] } } }","title":"Step 1: Hello message"},{"location":"arcaflow/concepts/plugin-protocol/#step-2-start-work-message","text":"The \u201cstart work\u201d message has the following parameters in CBOR: { \"id\" : \"id-of-the-step-to-execute\" , \"config\" : { // Input parameters according to schema here } } The plugin must respond with a CBOR message of the following format: { \"status\" : \"started\" }","title":"Step 2: Start work message"},{"location":"arcaflow/concepts/plugin-protocol/#step-3a-crash","text":"If the plugin execution ended unexpectedly, the plugin should crash and output a reasonable error message to the standard error. The plugin must exit with a non-zero exit status to notify the engine that the execution failed.","title":"Step 3/a: Crash"},{"location":"arcaflow/concepts/plugin-protocol/#step-3b-output","text":"When the plugin has executed successfully, it must emit a CBOR message to the standard output: { \"output_id\" : \"id-of-the-declared-output\" , \"output_data\" : { // Result data of the plugin }, \"debug_logs\" : \"Unstructured logs here for debugging as a string.\" }","title":"Step 3/b: Output"},{"location":"arcaflow/concepts/typing/","text":"The Arcaflow type system Work in Progress This document is work in progress and may change until the final release! Arcaflow takes a departure from the classic run-and-pray approach of running workloads and validates workflows for validity before executing them. To do this, Arcaflow starts the plugins as needed before the workflow is run and queries them for their schema . This schema will contain information about what kind of input a plugin requests and what kind of outputs it can produce. A plugin can support multiple workflow steps and must provide information about the data types in its input and output for each step. A step can have exactly one input format, but may declare more than one output. The typesystem is inspired by JSON schema and OpenAPI , but it is more restrictive due to the need to efficiently serialize workloads over various formats. Types The typing system supports the following data types. Objects are key-value pairs where the keys are always a fixed set of strings and values are of various types declared for each key. They are similar to classes in most programming languages. Fields in objects can be optional , which means they will have no value (commonly known as null , nil , or None ), or a default value. OneOf are a special type that is a union of multiple objects, distinguished by a special field called the discriminator. Lists are a sequence of values of the same type. The value type can be any of the other types described in this section. List items must always have a value and cannot be empty ( null , nil , or None ). Maps are key-value pairs that always have fixed types for both keys and values. Maps with mixed keys or values are not supported. Map keys can only be strings, integers, or enums. Map keys and values must always have a value and cannot be empty ( null , nil , or None ). Enums are either strings or integers that can take only a fixed set of values. Enums with mixed value types are not supported. Strings are a sequence of bytes. Patterns are regular expressions. Integers are 64-bit numbers that can take both positive and negative values. Floats are 64-bit floating point numbers that can take both positive and negative values. Booleans are values of true or false and cannot take any other values. Planned future types Timestamps are nanosecond-scale timestamp values for a fixed time in UTC. They are stored and transported as integers, but may be unserialized from strings too. Dates are calendar dates without timezone information. Times are the time of a day denominated as hours, minutes, seconds, etc. on a nanosecond scale. Datetimes are date and times together in one field. Durations are nanosecond-scale timespan values. UUIDs are UUID-formatted strings. Sets are an unordered collection of items that can only contain unique items. Validation The typing system also contains more in-depth validation than just simple types: Strings Strings can have a minimum or maximum length, as well as validation against a regular expression. Ints, floats Number types can have a minimum and maximum value (inclusive). Booleans Boolean types can take a value of either true or false , but when unserializing from YAML or JSON formats, strings or int values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 are also accepted. Lists, maps Lists and maps can have constraints on the minimum or maximum number of items in them (inclusive). Objects Object fields can have several constraints: required_if has a list of other fields that, if set, make the current field required. required_if_not has a list of other fields that, if none are set, make the current field required. conflicts has a list of other fields that cannot be set together with the current field. OneOf When you need to create a list of multiple object types, or simply have an either-or choice between two object types, you can use the OneOf type. This field uses an already existing field of the underlying objects, or adds an extra field to the schema to distinguish between the different types. Translated to JSON, you might see something like this: { \"_type\" : \"Greeter\" , \"message\" : \"Hello world!\" } Metadata Object fields can also declare metadata that will help with creating user interfaces for the object. These fields are: name : A user-readable name for the field. description : A user-readable description for the field. It may contain newlines, but no other formatting is allowed. icon : SVG icon Intent inference For display purposes, the type system is designed so that it can infer the intent of the data. We wish to communicate the following intents: Graphs are x-y values of timestamps mapped to one or more values. Log lines are timestamps associated with text. Events are timestamps associated with other structured data. We explicitly document the following inference rules, which will probably change in the future. A map with keys of timestamps and values of integers or floats is rendered as a graph. A map with keys of timestamps and values of objects consisting only of integers and floats is rendered as a graph. A map with keys of timestamps and values of strings is considered a log line. A map with keys of timestamps and objects that don\u2019t match the rules above are considered an event. A map with keys of short strings and integer or float values is considered a pie chart. A list of objects consisting of a single timestamp and otherwise only integers and floats is rendered as a graph. A list of objects with a single timestamp and a single string are considered a log line. A list of objects with a single short string and a single integer or float is considered a pie chart. A list of objects consisting of no more than one timestamp and multiple other fields not matching the rules above is considered an event. If an object has a field called \u201ctitle\u201d, \u201cname\u201d, or \u201clabel\u201d, it will be used as a label for the current data set in a chart, or as a title for the wrapping box for the user interface elements.","title":"The typing system"},{"location":"arcaflow/concepts/typing/#the-arcaflow-type-system","text":"Work in Progress This document is work in progress and may change until the final release! Arcaflow takes a departure from the classic run-and-pray approach of running workloads and validates workflows for validity before executing them. To do this, Arcaflow starts the plugins as needed before the workflow is run and queries them for their schema . This schema will contain information about what kind of input a plugin requests and what kind of outputs it can produce. A plugin can support multiple workflow steps and must provide information about the data types in its input and output for each step. A step can have exactly one input format, but may declare more than one output. The typesystem is inspired by JSON schema and OpenAPI , but it is more restrictive due to the need to efficiently serialize workloads over various formats.","title":"The Arcaflow type system"},{"location":"arcaflow/concepts/typing/#types","text":"The typing system supports the following data types. Objects are key-value pairs where the keys are always a fixed set of strings and values are of various types declared for each key. They are similar to classes in most programming languages. Fields in objects can be optional , which means they will have no value (commonly known as null , nil , or None ), or a default value. OneOf are a special type that is a union of multiple objects, distinguished by a special field called the discriminator. Lists are a sequence of values of the same type. The value type can be any of the other types described in this section. List items must always have a value and cannot be empty ( null , nil , or None ). Maps are key-value pairs that always have fixed types for both keys and values. Maps with mixed keys or values are not supported. Map keys can only be strings, integers, or enums. Map keys and values must always have a value and cannot be empty ( null , nil , or None ). Enums are either strings or integers that can take only a fixed set of values. Enums with mixed value types are not supported. Strings are a sequence of bytes. Patterns are regular expressions. Integers are 64-bit numbers that can take both positive and negative values. Floats are 64-bit floating point numbers that can take both positive and negative values. Booleans are values of true or false and cannot take any other values.","title":"Types"},{"location":"arcaflow/concepts/typing/#planned-future-types","text":"Timestamps are nanosecond-scale timestamp values for a fixed time in UTC. They are stored and transported as integers, but may be unserialized from strings too. Dates are calendar dates without timezone information. Times are the time of a day denominated as hours, minutes, seconds, etc. on a nanosecond scale. Datetimes are date and times together in one field. Durations are nanosecond-scale timespan values. UUIDs are UUID-formatted strings. Sets are an unordered collection of items that can only contain unique items.","title":"Planned future types"},{"location":"arcaflow/concepts/typing/#validation","text":"The typing system also contains more in-depth validation than just simple types:","title":"Validation"},{"location":"arcaflow/concepts/typing/#strings","text":"Strings can have a minimum or maximum length, as well as validation against a regular expression.","title":"Strings"},{"location":"arcaflow/concepts/typing/#ints-floats","text":"Number types can have a minimum and maximum value (inclusive).","title":"Ints, floats"},{"location":"arcaflow/concepts/typing/#booleans","text":"Boolean types can take a value of either true or false , but when unserializing from YAML or JSON formats, strings or int values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 are also accepted.","title":"Booleans"},{"location":"arcaflow/concepts/typing/#lists-maps","text":"Lists and maps can have constraints on the minimum or maximum number of items in them (inclusive).","title":"Lists, maps"},{"location":"arcaflow/concepts/typing/#objects","text":"Object fields can have several constraints: required_if has a list of other fields that, if set, make the current field required. required_if_not has a list of other fields that, if none are set, make the current field required. conflicts has a list of other fields that cannot be set together with the current field.","title":"Objects"},{"location":"arcaflow/concepts/typing/#oneof","text":"When you need to create a list of multiple object types, or simply have an either-or choice between two object types, you can use the OneOf type. This field uses an already existing field of the underlying objects, or adds an extra field to the schema to distinguish between the different types. Translated to JSON, you might see something like this: { \"_type\" : \"Greeter\" , \"message\" : \"Hello world!\" }","title":"OneOf"},{"location":"arcaflow/concepts/typing/#metadata","text":"Object fields can also declare metadata that will help with creating user interfaces for the object. These fields are: name : A user-readable name for the field. description : A user-readable description for the field. It may contain newlines, but no other formatting is allowed. icon : SVG icon","title":"Metadata"},{"location":"arcaflow/concepts/typing/#intent-inference","text":"For display purposes, the type system is designed so that it can infer the intent of the data. We wish to communicate the following intents: Graphs are x-y values of timestamps mapped to one or more values. Log lines are timestamps associated with text. Events are timestamps associated with other structured data. We explicitly document the following inference rules, which will probably change in the future. A map with keys of timestamps and values of integers or floats is rendered as a graph. A map with keys of timestamps and values of objects consisting only of integers and floats is rendered as a graph. A map with keys of timestamps and values of strings is considered a log line. A map with keys of timestamps and objects that don\u2019t match the rules above are considered an event. A map with keys of short strings and integer or float values is considered a pie chart. A list of objects consisting of a single timestamp and otherwise only integers and floats is rendered as a graph. A list of objects with a single timestamp and a single string are considered a log line. A list of objects with a single short string and a single integer or float is considered a pie chart. A list of objects consisting of no more than one timestamp and multiple other fields not matching the rules above is considered an event. If an object has a field called \u201ctitle\u201d, \u201cname\u201d, or \u201clabel\u201d, it will be used as a label for the current data set in a chart, or as a title for the wrapping box for the user interface elements.","title":"Intent inference"},{"location":"arcaflow/concepts/workflows/","text":"Arcaflow Workflows Work in Progress This document is work in progress and may change until the final release! Steps Workflows are a way to describe a sequence or parallel execution of individual steps. The steps are provided exclusively by plugins. The simplest workflow looks like this: stateDiagram-v2 [*] --> Step Step --> [*] However, this is only true if the step only has one output. Most steps will at least have two possible outputs, for success and error states: stateDiagram-v2 [*] --> Step Step --> [*]: yes Step --> [*]: no Plugins can declare as many outputs as needed, with custom names. The workflow engine doesn\u2019t make a distinction based on the names, all outputs are treated equal for execution. However, a few names are treated special for display purposes only: stateDiagram-v2 [*] --> Step Step --> [*]: success Step --> [*]: warning Step --> [*]: error These three output names ( success , warning , and error ) will be colored accordingly in the user interfaces. Other names may be used, but will not be colored. An important rule is that one step must always end in exactly one output. No step must end without an output, and no step can end in more than one output. This provides a mechanism to direct the flow of the workflow execution. Plugins must also explicitly declare what parameters they expect as input for the step, and the data types of these and what parameters they will produce as output. For more detaisl about this see the Type system page . Background processes Each plugin will only be invoked once, allowing plugins to run background processes, such as server applications. The plugins must handle SIGINT and SIGTERM events properly. Interconnecting steps When two steps are connected, they will be executed after each other: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2 Step2 --> [*] Similarly, when two steps are not directly connected, they may be executed in parallel: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2 Step1 --> [*] Step2 --> [*] You can use the interconnection to direct the flow of step outputs: stateDiagram-v2 Step1: Step 1 Step2: Step 2 Step3: Step 3 [*] --> Step1 Step1 --> Step2: success Step1 --> Step3: error Step2 --> [*] Step3 --> [*] Unconnected steps If you leave outputs unconnected, the workflow will be a failure and the engine will exit with a non-zero exit code. However, the engine will still attempt to finish whatever steps it can. Passing data between steps When two steps are connected, you have the ability to pass data between them. Emblematically described: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2: input_1 = $.step1.output_1 Step2 --> [*] The data type of the input on Step 2 in this case must match the result of the expression. If the data type does not match, the workflow will not be executed. Undefined inputs Step inputs can either be required or optional. When a step input is required, it must be configured or the workflow will fail to execute. However, there are cases when the inputs cannot be determined from previous steps. In this case, the workflow start can be connected and the required inputs can be obtained from the user when running the workflow: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2: input_1 = $.start.option_1 Step1 --> Step2: input_2 = $.step1.output_1 Step2 --> [*] This is typically the case when credentials, such as database access, etc. are required. Outputs The output for each step is preserved for later inspection. However, the workflow can explicitly declare outputs. These outputs are usable in scripted environments as a direct output of the workflow: stateDiagram-v2 [*] --> Step Step --> [*]: output_1 = $.step1.output_1 Execution environment Workflow plugins will always run in containers. You can configure these containers to run locally, or you can connect a remote execution environment. At this time the two environments that are supported are Docker (or Podman using the Docker compatibility API) and Kubernetes. Later on we plan to add container execution via SSH. A local Docker (or Docker-like) environment is always required. The workflow engine needs this to obtain schema information from the plugins, as well as for mirroring plugins to a network-disconnected environment. Plugins must also make sure that they can execute in an unprivileged container, even when they later on need to be executed in a privileged environment. This is needed to obtain the schema before executing it in the target environment. You can configure any step to run in a remote environment as long as the remote environment can pull the container image. The engine provides the facilities to mirror the required plugins into a disconnected environment. Plugins must not make network calls during startup, and they should come with everything they need to run built in, unless their specific purpose is to install something on step execution. The engine will execute the plugin container image in a network-disconnected environment at startup to obtain its schema. If it fails to execute without internet, the workflow will not run. The execution environment has further parameters. For Docker, these options are specific to Docker, for Kubernetes they are specific to Kubernetes. For Kubernetes, you can also specify constraints on where the step is executed. Flow control The workflow contains several flow control operations. These flow control operations are not implemented by plugins, but are part of the workflow engine itself. Abort The abort flow control is a quick way to exit out of a workflow. This is useful when entering a terminal error state and the workflow output data would be useless anyway. stateDiagram-v2 [*] --> Step1 Step1 --> Abort: Output 1 Step1 --> Step2: Output 2 Step2 --> [*] However, this is only required if you want to abort the workflow immediately. If you want an error case to result in the workflow failing, but whatever steps can be finished being finished, you can leave error outputs unconnected. Do-while A do-while block will execute the steps in it as long as a certain condition is met. The condition is derived from the output of the step or steps executed inside the loop: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: output_1_condition=$.step1.output_1.finished == false } DoWhile --> [*] If the step declares multiple outputs, multiple conditions are possible. The do-while block will also have multiple outputs: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: Output 1 condition Step1 --> [*]: Output 2 condition } DoWhile --> [*]: Output 1 DoWhile --> [*]: Output 2 You may decide to only allow exit from a loop if one of the two outputs is satisfied: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> Step1: Output 1 Step1 --> [*]: Output 2 } DoWhile --> [*]: Output 1 Condition A condition is a flow control operation that redirects the flow one way or another based on an expression. You can also create multiple branches to create a switch-case effect. stateDiagram-v2 state if_state <<choice>> Step1: Step 1 [*] --> Step1 Step1 --> if_state Step2: Step 2 Step3: Step 3 if_state --> Step2: $.step1.output_1 == true if_state --> Step3: $.step1.output_1 == false Multiply The multiply flow control operation is useful when you need to dynamically execute sub-workflows in parallel based on an input condition. You can, for example, use this to run a workflow step on multiple or all Kubernetes nodes. stateDiagram-v2 Lookup: Lookup Kubernetes hosts [*] --> Lookup Lookup --> Multiply state Multiply { [*] --> Stresstest Stresstest --> [*] } Multiply --> [*] The output of a Multiply operation will be a map, keyed with a string that is configured from the input. Tip You can think of a Multiply step like a for-each loop, but the steps being executed in parallel. Synchronize The synchronize step attempts to synchronize the execution of subsequent steps for a specified key. The key must be a constant and cannot be obtained from an input expression. stateDiagram-v2 [*] --> Step1 [*] --> Step2 Synchronize1: Synchronize (key=a) Synchronize2: Synchronize (key=a) Step1 --> Synchronize1 Step2 --> Synchronize2 Synchronize1 --> Step3 Synchronize2 --> Step4 Step3 --> [*] Step4 --> [*]","title":"Workflows"},{"location":"arcaflow/concepts/workflows/#arcaflow-workflows","text":"Work in Progress This document is work in progress and may change until the final release!","title":"Arcaflow Workflows"},{"location":"arcaflow/concepts/workflows/#steps","text":"Workflows are a way to describe a sequence or parallel execution of individual steps. The steps are provided exclusively by plugins. The simplest workflow looks like this: stateDiagram-v2 [*] --> Step Step --> [*] However, this is only true if the step only has one output. Most steps will at least have two possible outputs, for success and error states: stateDiagram-v2 [*] --> Step Step --> [*]: yes Step --> [*]: no Plugins can declare as many outputs as needed, with custom names. The workflow engine doesn\u2019t make a distinction based on the names, all outputs are treated equal for execution. However, a few names are treated special for display purposes only: stateDiagram-v2 [*] --> Step Step --> [*]: success Step --> [*]: warning Step --> [*]: error These three output names ( success , warning , and error ) will be colored accordingly in the user interfaces. Other names may be used, but will not be colored. An important rule is that one step must always end in exactly one output. No step must end without an output, and no step can end in more than one output. This provides a mechanism to direct the flow of the workflow execution. Plugins must also explicitly declare what parameters they expect as input for the step, and the data types of these and what parameters they will produce as output. For more detaisl about this see the Type system page .","title":"Steps"},{"location":"arcaflow/concepts/workflows/#background-processes","text":"Each plugin will only be invoked once, allowing plugins to run background processes, such as server applications. The plugins must handle SIGINT and SIGTERM events properly.","title":"Background processes"},{"location":"arcaflow/concepts/workflows/#interconnecting-steps","text":"When two steps are connected, they will be executed after each other: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2 Step2 --> [*] Similarly, when two steps are not directly connected, they may be executed in parallel: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2 Step1 --> [*] Step2 --> [*] You can use the interconnection to direct the flow of step outputs: stateDiagram-v2 Step1: Step 1 Step2: Step 2 Step3: Step 3 [*] --> Step1 Step1 --> Step2: success Step1 --> Step3: error Step2 --> [*] Step3 --> [*]","title":"Interconnecting steps"},{"location":"arcaflow/concepts/workflows/#unconnected-steps","text":"If you leave outputs unconnected, the workflow will be a failure and the engine will exit with a non-zero exit code. However, the engine will still attempt to finish whatever steps it can.","title":"Unconnected steps"},{"location":"arcaflow/concepts/workflows/#passing-data-between-steps","text":"When two steps are connected, you have the ability to pass data between them. Emblematically described: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2: input_1 = $.step1.output_1 Step2 --> [*] The data type of the input on Step 2 in this case must match the result of the expression. If the data type does not match, the workflow will not be executed.","title":"Passing data between steps"},{"location":"arcaflow/concepts/workflows/#undefined-inputs","text":"Step inputs can either be required or optional. When a step input is required, it must be configured or the workflow will fail to execute. However, there are cases when the inputs cannot be determined from previous steps. In this case, the workflow start can be connected and the required inputs can be obtained from the user when running the workflow: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2: input_1 = $.start.option_1 Step1 --> Step2: input_2 = $.step1.output_1 Step2 --> [*] This is typically the case when credentials, such as database access, etc. are required.","title":"Undefined inputs"},{"location":"arcaflow/concepts/workflows/#outputs","text":"The output for each step is preserved for later inspection. However, the workflow can explicitly declare outputs. These outputs are usable in scripted environments as a direct output of the workflow: stateDiagram-v2 [*] --> Step Step --> [*]: output_1 = $.step1.output_1","title":"Outputs"},{"location":"arcaflow/concepts/workflows/#execution-environment","text":"Workflow plugins will always run in containers. You can configure these containers to run locally, or you can connect a remote execution environment. At this time the two environments that are supported are Docker (or Podman using the Docker compatibility API) and Kubernetes. Later on we plan to add container execution via SSH. A local Docker (or Docker-like) environment is always required. The workflow engine needs this to obtain schema information from the plugins, as well as for mirroring plugins to a network-disconnected environment. Plugins must also make sure that they can execute in an unprivileged container, even when they later on need to be executed in a privileged environment. This is needed to obtain the schema before executing it in the target environment. You can configure any step to run in a remote environment as long as the remote environment can pull the container image. The engine provides the facilities to mirror the required plugins into a disconnected environment. Plugins must not make network calls during startup, and they should come with everything they need to run built in, unless their specific purpose is to install something on step execution. The engine will execute the plugin container image in a network-disconnected environment at startup to obtain its schema. If it fails to execute without internet, the workflow will not run. The execution environment has further parameters. For Docker, these options are specific to Docker, for Kubernetes they are specific to Kubernetes. For Kubernetes, you can also specify constraints on where the step is executed.","title":"Execution environment"},{"location":"arcaflow/concepts/workflows/#flow-control","text":"The workflow contains several flow control operations. These flow control operations are not implemented by plugins, but are part of the workflow engine itself.","title":"Flow control"},{"location":"arcaflow/concepts/workflows/#abort","text":"The abort flow control is a quick way to exit out of a workflow. This is useful when entering a terminal error state and the workflow output data would be useless anyway. stateDiagram-v2 [*] --> Step1 Step1 --> Abort: Output 1 Step1 --> Step2: Output 2 Step2 --> [*] However, this is only required if you want to abort the workflow immediately. If you want an error case to result in the workflow failing, but whatever steps can be finished being finished, you can leave error outputs unconnected.","title":"Abort"},{"location":"arcaflow/concepts/workflows/#do-while","text":"A do-while block will execute the steps in it as long as a certain condition is met. The condition is derived from the output of the step or steps executed inside the loop: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: output_1_condition=$.step1.output_1.finished == false } DoWhile --> [*] If the step declares multiple outputs, multiple conditions are possible. The do-while block will also have multiple outputs: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: Output 1 condition Step1 --> [*]: Output 2 condition } DoWhile --> [*]: Output 1 DoWhile --> [*]: Output 2 You may decide to only allow exit from a loop if one of the two outputs is satisfied: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> Step1: Output 1 Step1 --> [*]: Output 2 } DoWhile --> [*]: Output 1","title":"Do-while"},{"location":"arcaflow/concepts/workflows/#condition","text":"A condition is a flow control operation that redirects the flow one way or another based on an expression. You can also create multiple branches to create a switch-case effect. stateDiagram-v2 state if_state <<choice>> Step1: Step 1 [*] --> Step1 Step1 --> if_state Step2: Step 2 Step3: Step 3 if_state --> Step2: $.step1.output_1 == true if_state --> Step3: $.step1.output_1 == false","title":"Condition"},{"location":"arcaflow/concepts/workflows/#multiply","text":"The multiply flow control operation is useful when you need to dynamically execute sub-workflows in parallel based on an input condition. You can, for example, use this to run a workflow step on multiple or all Kubernetes nodes. stateDiagram-v2 Lookup: Lookup Kubernetes hosts [*] --> Lookup Lookup --> Multiply state Multiply { [*] --> Stresstest Stresstest --> [*] } Multiply --> [*] The output of a Multiply operation will be a map, keyed with a string that is configured from the input. Tip You can think of a Multiply step like a for-each loop, but the steps being executed in parallel.","title":"Multiply"},{"location":"arcaflow/concepts/workflows/#synchronize","text":"The synchronize step attempts to synchronize the execution of subsequent steps for a specified key. The key must be a constant and cannot be obtained from an input expression. stateDiagram-v2 [*] --> Step1 [*] --> Step2 Synchronize1: Synchronize (key=a) Synchronize2: Synchronize (key=a) Step1 --> Synchronize1 Step2 --> Synchronize2 Synchronize1 --> Step3 Synchronize2 --> Step4 Step3 --> [*] Step4 --> [*]","title":"Synchronize"},{"location":"arcaflow/creating-plugins/python/","text":"Creating plugins with Python If you want to create an Arcaflow plugin in Python, you will need three things: A container engine that can build images Python 3.9+ ( PyPy is supported) The Python SDK for Arcaflow plugins The easiest way is to start from the template repository for Python plugins , but starting from scratch is also fully supported. Before you start please familiarize yourself with the Arcaflow type system . Setting up your environment First, you will have to set up your environment. From the template repository Using pip Using Poetry Fork, then clone the template repository Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Create an empty folder. Create a requirements.txt with the following content: arcaflow-plugin-sdk Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Copy the example plugin , example config and the tests to your directory. Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Assuming you have Poetry installed, run the following command: poetry new your-plugin Then change the current directory to your-plugin . Figure out what the right command to call your Python version is: which python3.10 which python3.9 which python3 which python Make sure you have at least Python 3.9. Set Poetry to Python 3.9: poetry env use /path/to/your/python3.9 Check that your pyproject.toml file has the following lines: [tool.poetry.dependencies] python = \"^3.9\" Add the SDK as a dependency: poetry add arcaflow-plugin-sdk Copy the example plugin , example config and the tests to your directory. Activate the venv: poetry shell Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Now you are ready to start hacking away at your plugin! Creating your plugin the easy way A plugin is nothing but a list of functions with type-annotated parameters and decorators. For example, let\u2019s create a function: def pod_scenario ( input_parameter ): # Do pod scenario magic here However, this SDK uses Python type hints and decorators to automatically generate the schema required for Arcaflow. Alternatively, you can also build a schema by hand . The current section describes the automated way, the section below describes the manual way. Input parameters Your step function must take exactly one input parameter. This parameter must be a dataclass . For example: import dataclasses import re @dataclasses . dataclass class PodScenarioParams : namespace_pattern : re . Pattern = re . compile ( \".*\" ) pod_name_pattern : re . Pattern = re . compile ( \".*\" ) As you can see, our dataclass has two fields, each of which is a re.Pattern . This SDK automatically reads the types of the fields to construct the schema. See the Types section below for supported type patterns. Output parameters Now that you have your input parameter class, you must create one or more output classes in a similar fashion: import dataclasses import typing @dataclasses . dataclass class Pod : namespace : str name : str @dataclasses . dataclass class PodScenarioResults : pods_killed : typing . List [ Pod ] As you can see, your input may incorporate other classes, which themselves have to be dataclasses. Read on for more information on types . Creating a step function Now that we have both our input and output(s), let\u2019s go back to our initial pod_scenario function. Here we need to add a decorator to tell the SDK about metadata, and more importantly, what the return types are. (This is needed because Python does not support reading return types to an adequate level.) from arcaflow_plugin_sdk import plugin @plugin . step ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kill one or more pods matching the criteria\" , outputs = { \"success\" : PodScenarioResults , \"error\" : PodScenarioError }, ) def pod_scenario ( params : PodScenarioParams ): # Fail for now return \"error\" , PodScenarioError ( \"Not implemented\" ) As you can see, apart from the metadata, we also declare the type of the parameter object so the SDK can read it. Let\u2019s go through the @plugin.step decorator parameters one by one: id indicates the identifier of this step. This must be globally unique name indicates a human-readable name for this step description indicates a longer description for this step outputs indicates which possible outputs the step can have, with their output identifiers as keys The function must return the output identifier, along with the output object. Running the plugin Finally, we need to call plugin.run() in order to actually run the plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( plugin . build_schema ( # Pass one or more scenario functions here pod_scenario , ))) You can now call your plugin using ./yourscript.py -f path-to-parameters.yaml . If you have defined more than one step, you also need to pass the -s step-id parameter. Keep in mind, you should always test your plugin. See Testing your plugin below for details. Tip To prevent output from breaking the functionality when attached to the Arcaflow Engine, the SDK hides any output your step function writes to the standard output or standard error. You can use the --debug flag to show any output on the standard error in standalone mode. Types The SDK supports a wide range of types. Let\u2019s start with the basics: str int float bool Enums re.Pattern typing.List[othertype] (you must specify the type for the contents of the list) typing.Dict[keytype, valuetype] (you must specify the type for the keys and values) Any dataclass Optional parameters You can also declare any parameter as optional like this: @dataclasses . dataclass class MyClass : param : typing . Optional [ int ] = None Note that adding typing.Optional is not enough, you must specify the default value. Union types Union types are supported as long as all members are dataclasses. For example: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Union [ A , B ]] In the underlying transport a field name _type will be added to act as a serialization discriminator. You can also customize the discriminator field: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ A , B ], annotations . discriminator ( \"foo\" ) ] ] If you intend to use a non-string descriminator field, or you want to manually specify the discriminator value, you can do so by adding a discriminator_value annotation: @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ typing . Annotated [ A , annotations . discriminator_value ( \"first\" )], typing . Annotated [ B , annotations . discriminator_value ( \"second\" )] ], annotations . discriminator ( \"foo\" ) ] ] Tip You can add the discriminator field to your underlying dataclasses, but when present, their schema must match exactly . Validation You can also validate the values by using typing.Annotated , such as this: class MyClass : param : typing . Annotated [ int , schema . min ( 5 )] This will create a minimum-value validation for the parameter of 5. The following annotations are supported for validation: schema.min() for strings, ints, floats, lists, and maps schema.max() for strings, ints, floats, lists, and maps schema.pattern() for strings schema.required_if() for any field on an object schema.required_if_not() for any field on an object schema.conflicts() for any field on an object Metadata You can add metadata to your schema by using annotations @dataclasses . dataclass class MyClass : param : typing . Annotated [ str , schema . id ( \"my-param\" ), schema . name ( \"Parameter 1\" ), schema . description ( \"This is a parameter\" ), schema . icon ( \"<svg...>Add a 64x64 SVG here</svg>\" ) ] Default values You can add default values for your dataclass members like this: @dataclasses . dataclass class MyClass : param : str = \"this is the default value\" Units You can also include unit information in your schema. This will allow a user interface to treat your values accordingly: @dataclasses . dataclass class MyClass : param : typing . Annotated [ int , schema . units ( schema . UNIT_BYTE ), ] You can also define your own unit. For that, you have to define your base unit (e.g. \u201cbytes\u201d) and then the multipliers: UNIT_BYTE = schema . Units ( schema . Unit ( # Short, singular form: \"B\" , # Short, plural form: \"B\" , # Long, singular form: \"byte\" , # Long, plural form: \"bytes\" ), { 1024 : schema . Unit ( \"kB\" , \"kB\" , \"kilobyte\" , \"kilobytes\" ), 1048576 : schema . Unit ( \"MB\" , \"MB\" , \"megabyte\" , \"megabytes\" ), } ) This also allows you to parse strings like 5MB 4kB : parsed_unit : int = UNIT_BYTE . parse ( \"5MB 4 kB\" ) Conversely, you can also render the units: print ( UNIT_BYTE . format_short ( 5246976 )) Check the code completion for your units for more options. Examples You can also provide example values for your fields to help people providing the data: @dataclasses . dataclass class MyClass : param : typing . Annotated [ int , schema . example ( 1024 ), ] You can, of course, provide multiple examples too. Note, the example must be provided in its raw form (as dicts, lists, and scalars), not as dataclasses! Creating your plugin the hard way (not recommended) For performance reasons, or for the purposes of separation of concerns, you may want to create a schema by hand. This section walks you through declaring a schema by hand and then using it to call a function. Keep in mind, the SDK still primarily operates with dataclasses to transport structured data. However, we do not recommend this approach because it results in a lot of boilerplate code, and the real-world benefits are marginal at best. Also keep in mind, that your plugin will need more frequent updates if Arcaflow is extending the schema system and you want to switch to new versions. We start by defining a schema: from arcaflow_plugin_sdk import schema from typing import Dict steps : Dict [ str , schema . StepSchema ] s = schema . Schema ( steps , ) The steps parameter here must be a dict, where the key is the step ID and the value is the step schema. So, let\u2019s create a step schema: from arcaflow_plugin_sdk import schema step_schema = schema . StepSchema ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kills pods\" , input = input_schema , outputs = outputs , handler = my_handler_func ) Let\u2019s go in order: The input must be a schema of the type schema.ObjectType . This describes the single parameter that will be passed to my_handler_func . The outputs describe a Dict[str, schema.ObjectType] , where the key is the ID for the returned output type, while the value describes the output schema. The handler function takes one parameter, the object described in input and must return a tuple of a string and the output object. Here the ID uniquely identifies which output is intended, for example success and error , while the second parameter in the tuple must match the outputs declaration. That\u2019s it! Now all that\u2019s left is to define the ObjectType and any subobjects. ObjectType The ObjectType is intended as a backing type for dataclasses . For example: t = schema . ObjectType ( TestClass , { \"a\" : schema . Field ( type = schema . StringType (), required = True , ), \"b\" : schema . Field ( type = schema . IntType (), required = True , ) } ) The fields support the following parameters: type : underlying type schema for the field (required) name : name for the current field description : description for the current field required : marks the field as required required_if : a list of other fields that, if filled, will also cause the current field to be required required_if_not : a list of other fields that, if not set, will cause the current field to be required conflicts : a list of other fields that cannot be set together with the current field ScopeType and RefType Sometimes it is necessary to create circular references. This is where the ScopeType and the RefType comes into play. Scopes contain a list of objects that can be referenced by their ID, but one object is special: the root object of the scope. The RefType, on the other hand, is there to reference objects in a scope. Currently, the Python implementation passes the scope to the ref type directly, but the important rule is that ref types always reference their nearest scope up the tree. Do not create references that aim at scopes not directly above the ref! For example: @dataclasses . dataclass class OneOfData1 : a : str @dataclasses . dataclass class OneOfData2 : b : OneOfData1 scope = schema . ScopeType ( { \"OneOfData1\" : schema . ObjectType ( OneOfData1 , { \"a\" : schema . Field ( schema . StringType () ) } ), }, # Root object of scopes \"OneOfData2\" , ) scope . objects [ \"OneOfData2\" ] = schema . ObjectType ( OneOfData2 , { \"b\" : schema . Field ( schema . RefType ( \"OneOfData1\" , scope ) ) } ) As you can see, this API is not easy to use and is likely to change in the future. OneOfType The OneOfType allows you to create a type that is a combination of other ObjectTypes. When a value is deserialized, a special discriminator field is consulted to figure out which type is actually being sent. This discriminator field may be present in the underlying type. If it is, the type must match the declaration in the AnyOfType. For example: @dataclasses . dataclass class OneOfData1 : type : str a : str @dataclasses . dataclass class OneOfData2 : b : int scope = schema . ScopeType ( { \"OneOfData1\" : schema . ObjectType ( OneOfData1 , { # Here the discriminator field is also present in the underlying type \"type\" : schema . Field ( schema . StringType (), ), \"a\" : schema . Field ( schema . StringType () ) } ), \"OneOfData2\" : schema . ObjectType ( OneOfData2 , { \"b\" : schema . Field ( schema . IntType () ) } ) }, # Root object of scopes \"OneOfData1\" , ) s = schema . OneOfStringType ( { # Option 1 \"a\" : schema . RefType ( # The RefType resolves against the scope. \"OneOfData1\" , scope ), # Option 2 \"b\" : schema . RefType ( \"OneOfData2\" , scope ), }, # Pass the scope this type belongs do scope , # Discriminator field \"type\" , ) serialized_data = s . serialize ( OneOfData1 ( \"a\" , \"Hello world!\" )) pprint . pprint ( serialized_data ) Note, that the OneOfTypes take all object-like elements, such as refs, objects, or scopes. StringType String types indicate that the underlying type is a string. t = schema . StringType () The string type supports the following parameters: min_length : minimum length for the string (inclusive) max_length : maximum length for the string (inclusive) pattern : regular expression the string must match PatternType The pattern type indicates that the field must contain a regular expression. It will be decoded as re.Pattern . t = schema . PatternType () The pattern type has no parameters. IntType The int type indicates that the underlying type is an integer. t = schema . IntType () The int type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive). FloatType The float type indicates that the underlying type is a floating point number. t = schema . FloatType () The float type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive). BoolType The bool type indicates that the underlying value is a boolean. When unserializing, this type also supports string and integer values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 . EnumType The enum type creates a type from an existing enum: class MyEnum ( Enum ): A = \"a\" B = \"b\" t = schema . EnumType ( MyEnum ) The enum type has no further parameters. ListType The list type describes a list of items. The item type must be described: t = schema . ListType ( schema . StringType () ) The list type supports the following extra parameters: min : The minimum number of items in the list (inclusive) max : The maximum number of items in the list (inclusive) MapType The map type describes a key-value type (dict). You must specify both the key and the value type: t = schema . MapType ( schema . StringType (), schema . StringType () ) The map type supports the following extra parameters: min : The minimum number of items in the map (inclusive) max : The maximum number of items in the map (inclusive) Running the plugin If you create the schema by hand, you can add the following code to your plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( your_schema )) You can then run your plugin as described before. Testing your plugin You should always make sure you have enough test coverage to prevent your plugin from breaking. To help you with testing, this SDK provides some tools for testing: Serialization tests for your input and output to make sure your classes can be serialized for transport Functional tests that call your plugin and make sure it works correctly Writing a serialization test You can use any test framework you like for your serialization test, we\u2019ll demonstrate with unittest as it is included directly in Python. The key to this test is to call plugin.test_object_serialization() with an instance of your dataclass that you want to test: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): self . assertTrue ( plugin . test_object_serialization ( example_plugin . PodScenarioResults ( [ example_plugin . Pod ( namespace = \"default\" , name = \"nginx-asdf\" ) ] ) )) Remember, you need to call this function with an instance containing actual data, not just the class name. The test function will first serialize, then unserialize your data and check if it\u2019s the same. If you want to use a manually created schema, you can do so, too: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): plugin . test_object_serialization ( example_plugin . PodScenarioResults ( #... ), schema . ObjectType ( #... ) ) Functional tests Functional tests don\u2019t have anything special about them. You can directly call your code with your dataclasses as parameters, and check the return. This works best on auto-generated schemas with the @plugin.step decorator. See below for manually created schemas. class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): input = example_plugin . PodScenarioParams () output_id , output_data = example_plugin . pod_scenario ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) ) If you created your schema manually, the best way to write your tests is to include the schema in your test. This will automatically validate both the input and the output, making sure they conform to your schema. For example: class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): step_schema = schema . StepSchema ( #... handler = example_plugin . pod_scenario , ) input = example_plugin . PodScenarioParams () output_id , output_data = step_schema ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) ) Embedding your plugin Instead of using your plugin as a standalone tool or in conjunction with Arcaflow, you can also embed your plugin into your existing Python application. To do that you simply build a schema using one of the methods described above and then call the schema yourself. You can pass raw data as an input, and you\u2019ll get the benefit of schema validation. # Build your schema using the schema builder from above with the step functions passed. schema = plugin . build_schema ( pod_scenario ) # Which step we want to execute step_id = \"pod\" # Input parameters. Note, these must be a dict, not a dataclass step_params = { \"pod_name_pattern\" : \".*\" , \"pod_namespace_pattern\" : \".*\" , } # Execute the step output_id , output_data = schema ( step_id , step_params ) # Print which kind of result we have pprint . pprint ( output_id ) # Print the result data pprint . pprint ( output_data ) However, the example above requires you to provide the data as a dict , not a dataclass , and it will also return a dict as an output object. Sometimes, you may want to use a partial approach, where you only use part of the SDK. In this case, you can change your code to run any of the following functions, in order: serialization.load_from_file() to load a YAML or JSON file into a dict yourschema.unserialize_input() to turn a dict into a dataclass needed for your steps yourschema.call_step() to run a step with the unserialized dataclass yourschema.serialize_output() to turn the output dataclass into a dict FAQ How can I add a field with dashes, such as my-field ? Dataclasses don\u2019t support dashes in parameters. You can work around this by defining the id annotation: @dataclasses . dataclass class MyData : my_field : typing . Annotated [ str , schema . id ( \"my-field\" ), ] How can I write a dataclass from a schema to a YAML or JSON file? You can extend Pythons JSON encoder to support dataclasses. If that doesn\u2019t suit your needs, you can use this SDK to convert the dataclasses to their basic representations and then write that to your JSON or YAML file. First, add this outside of your step: my_object_schema = plugin . build_object_schema ( YourDataclass ) Inside your step function you can then dump the data from your input def your_step ( params : YourParams ) yaml_contents = yaml . dump ( my_object_schema . serialize ( params . some_param )) How can I easily load a list from a YAML or JSON into a list of dataclasses? This requires a bit of trickery. First, we build a schema from the dataclass representing the row or entry in the list: my_row_schema = plugin . build_object_schema ( MyRow ) Then you can create a list schema: my_list_schema = schema . ListType ( my_row_schema ) You can now unserialize a list obtained from the YAML or JSON file: my_data = my_list_schema . unserialize ( json . loads ( ... ))","title":"Python"},{"location":"arcaflow/creating-plugins/python/#creating-plugins-with-python","text":"If you want to create an Arcaflow plugin in Python, you will need three things: A container engine that can build images Python 3.9+ ( PyPy is supported) The Python SDK for Arcaflow plugins The easiest way is to start from the template repository for Python plugins , but starting from scratch is also fully supported. Before you start please familiarize yourself with the Arcaflow type system .","title":"Creating plugins with Python"},{"location":"arcaflow/creating-plugins/python/#setting-up-your-environment","text":"First, you will have to set up your environment. From the template repository Using pip Using Poetry Fork, then clone the template repository Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Create an empty folder. Create a requirements.txt with the following content: arcaflow-plugin-sdk Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Copy the example plugin , example config and the tests to your directory. Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Assuming you have Poetry installed, run the following command: poetry new your-plugin Then change the current directory to your-plugin . Figure out what the right command to call your Python version is: which python3.10 which python3.9 which python3 which python Make sure you have at least Python 3.9. Set Poetry to Python 3.9: poetry env use /path/to/your/python3.9 Check that your pyproject.toml file has the following lines: [tool.poetry.dependencies] python = \"^3.9\" Add the SDK as a dependency: poetry add arcaflow-plugin-sdk Copy the example plugin , example config and the tests to your directory. Activate the venv: poetry shell Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Now you are ready to start hacking away at your plugin!","title":"Setting up your environment"},{"location":"arcaflow/creating-plugins/python/#creating-your-plugin-the-easy-way","text":"A plugin is nothing but a list of functions with type-annotated parameters and decorators. For example, let\u2019s create a function: def pod_scenario ( input_parameter ): # Do pod scenario magic here However, this SDK uses Python type hints and decorators to automatically generate the schema required for Arcaflow. Alternatively, you can also build a schema by hand . The current section describes the automated way, the section below describes the manual way.","title":"Creating your plugin the easy way"},{"location":"arcaflow/creating-plugins/python/#input-parameters","text":"Your step function must take exactly one input parameter. This parameter must be a dataclass . For example: import dataclasses import re @dataclasses . dataclass class PodScenarioParams : namespace_pattern : re . Pattern = re . compile ( \".*\" ) pod_name_pattern : re . Pattern = re . compile ( \".*\" ) As you can see, our dataclass has two fields, each of which is a re.Pattern . This SDK automatically reads the types of the fields to construct the schema. See the Types section below for supported type patterns.","title":"Input parameters"},{"location":"arcaflow/creating-plugins/python/#output-parameters","text":"Now that you have your input parameter class, you must create one or more output classes in a similar fashion: import dataclasses import typing @dataclasses . dataclass class Pod : namespace : str name : str @dataclasses . dataclass class PodScenarioResults : pods_killed : typing . List [ Pod ] As you can see, your input may incorporate other classes, which themselves have to be dataclasses. Read on for more information on types .","title":"Output parameters"},{"location":"arcaflow/creating-plugins/python/#creating-a-step-function","text":"Now that we have both our input and output(s), let\u2019s go back to our initial pod_scenario function. Here we need to add a decorator to tell the SDK about metadata, and more importantly, what the return types are. (This is needed because Python does not support reading return types to an adequate level.) from arcaflow_plugin_sdk import plugin @plugin . step ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kill one or more pods matching the criteria\" , outputs = { \"success\" : PodScenarioResults , \"error\" : PodScenarioError }, ) def pod_scenario ( params : PodScenarioParams ): # Fail for now return \"error\" , PodScenarioError ( \"Not implemented\" ) As you can see, apart from the metadata, we also declare the type of the parameter object so the SDK can read it. Let\u2019s go through the @plugin.step decorator parameters one by one: id indicates the identifier of this step. This must be globally unique name indicates a human-readable name for this step description indicates a longer description for this step outputs indicates which possible outputs the step can have, with their output identifiers as keys The function must return the output identifier, along with the output object.","title":"Creating a step function"},{"location":"arcaflow/creating-plugins/python/#running-the-plugin","text":"Finally, we need to call plugin.run() in order to actually run the plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( plugin . build_schema ( # Pass one or more scenario functions here pod_scenario , ))) You can now call your plugin using ./yourscript.py -f path-to-parameters.yaml . If you have defined more than one step, you also need to pass the -s step-id parameter. Keep in mind, you should always test your plugin. See Testing your plugin below for details. Tip To prevent output from breaking the functionality when attached to the Arcaflow Engine, the SDK hides any output your step function writes to the standard output or standard error. You can use the --debug flag to show any output on the standard error in standalone mode.","title":"Running the plugin"},{"location":"arcaflow/creating-plugins/python/#types","text":"The SDK supports a wide range of types. Let\u2019s start with the basics: str int float bool Enums re.Pattern typing.List[othertype] (you must specify the type for the contents of the list) typing.Dict[keytype, valuetype] (you must specify the type for the keys and values) Any dataclass","title":"Types"},{"location":"arcaflow/creating-plugins/python/#optional-parameters","text":"You can also declare any parameter as optional like this: @dataclasses . dataclass class MyClass : param : typing . Optional [ int ] = None Note that adding typing.Optional is not enough, you must specify the default value.","title":"Optional parameters"},{"location":"arcaflow/creating-plugins/python/#union-types","text":"Union types are supported as long as all members are dataclasses. For example: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Union [ A , B ]] In the underlying transport a field name _type will be added to act as a serialization discriminator. You can also customize the discriminator field: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ A , B ], annotations . discriminator ( \"foo\" ) ] ] If you intend to use a non-string descriminator field, or you want to manually specify the discriminator value, you can do so by adding a discriminator_value annotation: @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ typing . Annotated [ A , annotations . discriminator_value ( \"first\" )], typing . Annotated [ B , annotations . discriminator_value ( \"second\" )] ], annotations . discriminator ( \"foo\" ) ] ] Tip You can add the discriminator field to your underlying dataclasses, but when present, their schema must match exactly .","title":"Union types"},{"location":"arcaflow/creating-plugins/python/#validation","text":"You can also validate the values by using typing.Annotated , such as this: class MyClass : param : typing . Annotated [ int , schema . min ( 5 )] This will create a minimum-value validation for the parameter of 5. The following annotations are supported for validation: schema.min() for strings, ints, floats, lists, and maps schema.max() for strings, ints, floats, lists, and maps schema.pattern() for strings schema.required_if() for any field on an object schema.required_if_not() for any field on an object schema.conflicts() for any field on an object","title":"Validation"},{"location":"arcaflow/creating-plugins/python/#metadata","text":"You can add metadata to your schema by using annotations @dataclasses . dataclass class MyClass : param : typing . Annotated [ str , schema . id ( \"my-param\" ), schema . name ( \"Parameter 1\" ), schema . description ( \"This is a parameter\" ), schema . icon ( \"<svg...>Add a 64x64 SVG here</svg>\" ) ]","title":"Metadata"},{"location":"arcaflow/creating-plugins/python/#default-values","text":"You can add default values for your dataclass members like this: @dataclasses . dataclass class MyClass : param : str = \"this is the default value\"","title":"Default values"},{"location":"arcaflow/creating-plugins/python/#units","text":"You can also include unit information in your schema. This will allow a user interface to treat your values accordingly: @dataclasses . dataclass class MyClass : param : typing . Annotated [ int , schema . units ( schema . UNIT_BYTE ), ] You can also define your own unit. For that, you have to define your base unit (e.g. \u201cbytes\u201d) and then the multipliers: UNIT_BYTE = schema . Units ( schema . Unit ( # Short, singular form: \"B\" , # Short, plural form: \"B\" , # Long, singular form: \"byte\" , # Long, plural form: \"bytes\" ), { 1024 : schema . Unit ( \"kB\" , \"kB\" , \"kilobyte\" , \"kilobytes\" ), 1048576 : schema . Unit ( \"MB\" , \"MB\" , \"megabyte\" , \"megabytes\" ), } ) This also allows you to parse strings like 5MB 4kB : parsed_unit : int = UNIT_BYTE . parse ( \"5MB 4 kB\" ) Conversely, you can also render the units: print ( UNIT_BYTE . format_short ( 5246976 )) Check the code completion for your units for more options.","title":"Units"},{"location":"arcaflow/creating-plugins/python/#examples","text":"You can also provide example values for your fields to help people providing the data: @dataclasses . dataclass class MyClass : param : typing . Annotated [ int , schema . example ( 1024 ), ] You can, of course, provide multiple examples too. Note, the example must be provided in its raw form (as dicts, lists, and scalars), not as dataclasses!","title":"Examples"},{"location":"arcaflow/creating-plugins/python/#creating-your-plugin-the-hard-way-not-recommended","text":"For performance reasons, or for the purposes of separation of concerns, you may want to create a schema by hand. This section walks you through declaring a schema by hand and then using it to call a function. Keep in mind, the SDK still primarily operates with dataclasses to transport structured data. However, we do not recommend this approach because it results in a lot of boilerplate code, and the real-world benefits are marginal at best. Also keep in mind, that your plugin will need more frequent updates if Arcaflow is extending the schema system and you want to switch to new versions. We start by defining a schema: from arcaflow_plugin_sdk import schema from typing import Dict steps : Dict [ str , schema . StepSchema ] s = schema . Schema ( steps , ) The steps parameter here must be a dict, where the key is the step ID and the value is the step schema. So, let\u2019s create a step schema: from arcaflow_plugin_sdk import schema step_schema = schema . StepSchema ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kills pods\" , input = input_schema , outputs = outputs , handler = my_handler_func ) Let\u2019s go in order: The input must be a schema of the type schema.ObjectType . This describes the single parameter that will be passed to my_handler_func . The outputs describe a Dict[str, schema.ObjectType] , where the key is the ID for the returned output type, while the value describes the output schema. The handler function takes one parameter, the object described in input and must return a tuple of a string and the output object. Here the ID uniquely identifies which output is intended, for example success and error , while the second parameter in the tuple must match the outputs declaration. That\u2019s it! Now all that\u2019s left is to define the ObjectType and any subobjects.","title":"Creating your plugin the hard way (not recommended)"},{"location":"arcaflow/creating-plugins/python/#objecttype","text":"The ObjectType is intended as a backing type for dataclasses . For example: t = schema . ObjectType ( TestClass , { \"a\" : schema . Field ( type = schema . StringType (), required = True , ), \"b\" : schema . Field ( type = schema . IntType (), required = True , ) } ) The fields support the following parameters: type : underlying type schema for the field (required) name : name for the current field description : description for the current field required : marks the field as required required_if : a list of other fields that, if filled, will also cause the current field to be required required_if_not : a list of other fields that, if not set, will cause the current field to be required conflicts : a list of other fields that cannot be set together with the current field","title":"ObjectType"},{"location":"arcaflow/creating-plugins/python/#scopetype-and-reftype","text":"Sometimes it is necessary to create circular references. This is where the ScopeType and the RefType comes into play. Scopes contain a list of objects that can be referenced by their ID, but one object is special: the root object of the scope. The RefType, on the other hand, is there to reference objects in a scope. Currently, the Python implementation passes the scope to the ref type directly, but the important rule is that ref types always reference their nearest scope up the tree. Do not create references that aim at scopes not directly above the ref! For example: @dataclasses . dataclass class OneOfData1 : a : str @dataclasses . dataclass class OneOfData2 : b : OneOfData1 scope = schema . ScopeType ( { \"OneOfData1\" : schema . ObjectType ( OneOfData1 , { \"a\" : schema . Field ( schema . StringType () ) } ), }, # Root object of scopes \"OneOfData2\" , ) scope . objects [ \"OneOfData2\" ] = schema . ObjectType ( OneOfData2 , { \"b\" : schema . Field ( schema . RefType ( \"OneOfData1\" , scope ) ) } ) As you can see, this API is not easy to use and is likely to change in the future.","title":"ScopeType and RefType"},{"location":"arcaflow/creating-plugins/python/#oneoftype","text":"The OneOfType allows you to create a type that is a combination of other ObjectTypes. When a value is deserialized, a special discriminator field is consulted to figure out which type is actually being sent. This discriminator field may be present in the underlying type. If it is, the type must match the declaration in the AnyOfType. For example: @dataclasses . dataclass class OneOfData1 : type : str a : str @dataclasses . dataclass class OneOfData2 : b : int scope = schema . ScopeType ( { \"OneOfData1\" : schema . ObjectType ( OneOfData1 , { # Here the discriminator field is also present in the underlying type \"type\" : schema . Field ( schema . StringType (), ), \"a\" : schema . Field ( schema . StringType () ) } ), \"OneOfData2\" : schema . ObjectType ( OneOfData2 , { \"b\" : schema . Field ( schema . IntType () ) } ) }, # Root object of scopes \"OneOfData1\" , ) s = schema . OneOfStringType ( { # Option 1 \"a\" : schema . RefType ( # The RefType resolves against the scope. \"OneOfData1\" , scope ), # Option 2 \"b\" : schema . RefType ( \"OneOfData2\" , scope ), }, # Pass the scope this type belongs do scope , # Discriminator field \"type\" , ) serialized_data = s . serialize ( OneOfData1 ( \"a\" , \"Hello world!\" )) pprint . pprint ( serialized_data ) Note, that the OneOfTypes take all object-like elements, such as refs, objects, or scopes.","title":"OneOfType"},{"location":"arcaflow/creating-plugins/python/#stringtype","text":"String types indicate that the underlying type is a string. t = schema . StringType () The string type supports the following parameters: min_length : minimum length for the string (inclusive) max_length : maximum length for the string (inclusive) pattern : regular expression the string must match","title":"StringType"},{"location":"arcaflow/creating-plugins/python/#patterntype","text":"The pattern type indicates that the field must contain a regular expression. It will be decoded as re.Pattern . t = schema . PatternType () The pattern type has no parameters.","title":"PatternType"},{"location":"arcaflow/creating-plugins/python/#inttype","text":"The int type indicates that the underlying type is an integer. t = schema . IntType () The int type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive).","title":"IntType"},{"location":"arcaflow/creating-plugins/python/#floattype","text":"The float type indicates that the underlying type is a floating point number. t = schema . FloatType () The float type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive).","title":"FloatType"},{"location":"arcaflow/creating-plugins/python/#booltype","text":"The bool type indicates that the underlying value is a boolean. When unserializing, this type also supports string and integer values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 .","title":"BoolType"},{"location":"arcaflow/creating-plugins/python/#enumtype","text":"The enum type creates a type from an existing enum: class MyEnum ( Enum ): A = \"a\" B = \"b\" t = schema . EnumType ( MyEnum ) The enum type has no further parameters.","title":"EnumType"},{"location":"arcaflow/creating-plugins/python/#listtype","text":"The list type describes a list of items. The item type must be described: t = schema . ListType ( schema . StringType () ) The list type supports the following extra parameters: min : The minimum number of items in the list (inclusive) max : The maximum number of items in the list (inclusive)","title":"ListType"},{"location":"arcaflow/creating-plugins/python/#maptype","text":"The map type describes a key-value type (dict). You must specify both the key and the value type: t = schema . MapType ( schema . StringType (), schema . StringType () ) The map type supports the following extra parameters: min : The minimum number of items in the map (inclusive) max : The maximum number of items in the map (inclusive)","title":"MapType"},{"location":"arcaflow/creating-plugins/python/#running-the-plugin_1","text":"If you create the schema by hand, you can add the following code to your plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( your_schema )) You can then run your plugin as described before.","title":"Running the plugin"},{"location":"arcaflow/creating-plugins/python/#testing-your-plugin","text":"You should always make sure you have enough test coverage to prevent your plugin from breaking. To help you with testing, this SDK provides some tools for testing: Serialization tests for your input and output to make sure your classes can be serialized for transport Functional tests that call your plugin and make sure it works correctly","title":"Testing your plugin"},{"location":"arcaflow/creating-plugins/python/#writing-a-serialization-test","text":"You can use any test framework you like for your serialization test, we\u2019ll demonstrate with unittest as it is included directly in Python. The key to this test is to call plugin.test_object_serialization() with an instance of your dataclass that you want to test: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): self . assertTrue ( plugin . test_object_serialization ( example_plugin . PodScenarioResults ( [ example_plugin . Pod ( namespace = \"default\" , name = \"nginx-asdf\" ) ] ) )) Remember, you need to call this function with an instance containing actual data, not just the class name. The test function will first serialize, then unserialize your data and check if it\u2019s the same. If you want to use a manually created schema, you can do so, too: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): plugin . test_object_serialization ( example_plugin . PodScenarioResults ( #... ), schema . ObjectType ( #... ) )","title":"Writing a serialization test"},{"location":"arcaflow/creating-plugins/python/#functional-tests","text":"Functional tests don\u2019t have anything special about them. You can directly call your code with your dataclasses as parameters, and check the return. This works best on auto-generated schemas with the @plugin.step decorator. See below for manually created schemas. class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): input = example_plugin . PodScenarioParams () output_id , output_data = example_plugin . pod_scenario ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) ) If you created your schema manually, the best way to write your tests is to include the schema in your test. This will automatically validate both the input and the output, making sure they conform to your schema. For example: class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): step_schema = schema . StepSchema ( #... handler = example_plugin . pod_scenario , ) input = example_plugin . PodScenarioParams () output_id , output_data = step_schema ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) )","title":"Functional tests"},{"location":"arcaflow/creating-plugins/python/#embedding-your-plugin","text":"Instead of using your plugin as a standalone tool or in conjunction with Arcaflow, you can also embed your plugin into your existing Python application. To do that you simply build a schema using one of the methods described above and then call the schema yourself. You can pass raw data as an input, and you\u2019ll get the benefit of schema validation. # Build your schema using the schema builder from above with the step functions passed. schema = plugin . build_schema ( pod_scenario ) # Which step we want to execute step_id = \"pod\" # Input parameters. Note, these must be a dict, not a dataclass step_params = { \"pod_name_pattern\" : \".*\" , \"pod_namespace_pattern\" : \".*\" , } # Execute the step output_id , output_data = schema ( step_id , step_params ) # Print which kind of result we have pprint . pprint ( output_id ) # Print the result data pprint . pprint ( output_data ) However, the example above requires you to provide the data as a dict , not a dataclass , and it will also return a dict as an output object. Sometimes, you may want to use a partial approach, where you only use part of the SDK. In this case, you can change your code to run any of the following functions, in order: serialization.load_from_file() to load a YAML or JSON file into a dict yourschema.unserialize_input() to turn a dict into a dataclass needed for your steps yourschema.call_step() to run a step with the unserialized dataclass yourschema.serialize_output() to turn the output dataclass into a dict","title":"Embedding your plugin"},{"location":"arcaflow/creating-plugins/python/#faq","text":"","title":"FAQ"},{"location":"arcaflow/creating-plugins/python/#how-can-i-add-a-field-with-dashes-such-as-my-field","text":"Dataclasses don\u2019t support dashes in parameters. You can work around this by defining the id annotation: @dataclasses . dataclass class MyData : my_field : typing . Annotated [ str , schema . id ( \"my-field\" ), ]","title":"How can I add a field with dashes, such as my-field?"},{"location":"arcaflow/creating-plugins/python/#how-can-i-write-a-dataclass-from-a-schema-to-a-yaml-or-json-file","text":"You can extend Pythons JSON encoder to support dataclasses. If that doesn\u2019t suit your needs, you can use this SDK to convert the dataclasses to their basic representations and then write that to your JSON or YAML file. First, add this outside of your step: my_object_schema = plugin . build_object_schema ( YourDataclass ) Inside your step function you can then dump the data from your input def your_step ( params : YourParams ) yaml_contents = yaml . dump ( my_object_schema . serialize ( params . some_param ))","title":"How can I write a dataclass from a schema to a YAML or JSON file?"},{"location":"arcaflow/creating-plugins/python/#how-can-i-easily-load-a-list-from-a-yaml-or-json-into-a-list-of-dataclasses","text":"This requires a bit of trickery. First, we build a schema from the dataclass representing the row or entry in the list: my_row_schema = plugin . build_object_schema ( MyRow ) Then you can create a list schema: my_list_schema = schema . ListType ( my_row_schema ) You can now unserialize a list obtained from the YAML or JSON file: my_data = my_list_schema . unserialize ( json . loads ( ... ))","title":"How can I easily load a list from a YAML or JSON into a list of dataclasses?"},{"location":"arcalog/","text":"Arcalog: Assisted Root Cause Analysis for Your Logs Arcalog is still in early development. A scientific paper describing the project is available as a preprint . The README contains a guide on how to use Arcalog to gather data as well as how to use the --http flag to run a minimal user interface for downloading individual build IDs from Prow. Pre-release developer documentation is also available if you want to use the early pre-release version of Arcalog to embed the data gathering steps into your own application.","title":"Arcalog"},{"location":"arcalog/#arcalog-assisted-root-cause-analysis-for-your-logs","text":"Arcalog is still in early development. A scientific paper describing the project is available as a preprint . The README contains a guide on how to use Arcalog to gather data as well as how to use the --http flag to run a minimal user interface for downloading individual build IDs from Prow. Pre-release developer documentation is also available if you want to use the early pre-release version of Arcalog to embed the data gathering steps into your own application.","title":"Arcalog: Assisted Root Cause Analysis for Your Logs"}]}