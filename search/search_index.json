{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Arcalot: Another Repository Containing A Lot Of Things The Arcalot community develops tools, plugins, and libraries that you can use either standalone as a library, and/or via a user interface or CLI. You can run the tools locally, remotely, or as part of a bigger system. Arcalot: Helps you create workflows with normalized input and output schemas Provides you with assisted and automated root cause analysis for the workflows you create as well as CI and other log systems Provides stable plugins for several workloads Arcaflow Arcaflow is a workflow engine consisting of three main components: Core engine UI Plugins (including SDKs for Go and Python to write your own plugins) It allows you to click and drag plugins into a workflow for your systems and, if needed, feed the resulting data back into the UI for further analysis. You can also use it just to generate a workflow with parallel and subsequent tasks via the command line. There is a range of supported plugins, written either in Go or Python. Read more Arcalog Arcalog can assist you with or automate your root cause analysis in CI or other log systems either as a standalone tool or by embedding it into your applications. It also provides additional tooling to download jobs from various log systems or add your own log files for analysis. Read more Community You can find our general community health files like our code of conduct and contribution guidelines in the .github repository . If you have any questions or suggestions, please use the issues in the respective repository or contribute to the discussions . If you would like to contribute, check out the issues in the individual repositories and our project boards where we organize our work.","title":"Home"},{"location":"#arcalot-another-repository-containing-a-lot-of-things","text":"The Arcalot community develops tools, plugins, and libraries that you can use either standalone as a library, and/or via a user interface or CLI. You can run the tools locally, remotely, or as part of a bigger system. Arcalot: Helps you create workflows with normalized input and output schemas Provides you with assisted and automated root cause analysis for the workflows you create as well as CI and other log systems Provides stable plugins for several workloads","title":"Arcalot: Another Repository Containing A Lot Of Things"},{"location":"#arcaflow","text":"Arcaflow is a workflow engine consisting of three main components: Core engine UI Plugins (including SDKs for Go and Python to write your own plugins) It allows you to click and drag plugins into a workflow for your systems and, if needed, feed the resulting data back into the UI for further analysis. You can also use it just to generate a workflow with parallel and subsequent tasks via the command line. There is a range of supported plugins, written either in Go or Python. Read more","title":"Arcaflow"},{"location":"#arcalog","text":"Arcalog can assist you with or automate your root cause analysis in CI or other log systems either as a standalone tool or by embedding it into your applications. It also provides additional tooling to download jobs from various log systems or add your own log files for analysis. Read more","title":"Arcalog"},{"location":"#community","text":"You can find our general community health files like our code of conduct and contribution guidelines in the .github repository . If you have any questions or suggestions, please use the issues in the respective repository or contribute to the discussions . If you would like to contribute, check out the issues in the individual repositories and our project boards where we organize our work.","title":"Community"},{"location":"arcaflow/","text":"Arcaflow: The noble workflow engine","title":"Arcaflow: The noble workflow engine"},{"location":"arcaflow/#arcaflow-the-noble-workflow-engine","text":"","title":"Arcaflow: The noble workflow engine"},{"location":"arcaflow/concepts/typing/","text":"The Arcaflow type system Arcaflow takes a departure from the classic run-and-pray approach of running workloads and validates workflows for validity before executing them. To do this, Arcaflow starts the plugins as needed before the workflow is run and queries them for their schema . This schema will contain information about what kind of input a plugin requests and what kind of outputs it can produce. A plugin can support multiple workflow steps and must provide information about the data types in its input and output for each step. A step can have exactly one input format, but may declare more than one output. The typesystem is inspired by JSON schema and OpenAPI , but it is more restrictive due to the need to efficiently serialize workloads over various formats. Types The typing system supports the following data types. Objects are key-value pairs where the keys are always a fixed set of strings and values are of various types declared for each key. They are similar to classes in most programming languages. Fields in objects can be optional , which means they will have no value (commonly known as null , nil , or None ), or a default value. OneOf are a special type that is a union of multiple objects, distinguished by a special field called the discriminator. Lists are a sequence of values of the same type. The value type can be any of the other types described in this section. List items must always have a value and cannot be empty ( null , nil , or None ). Maps are key-value pairs that always have fixed types for both keys and values. Maps with mixed keys or values are not supported. Map keys can only be strings, integers, or enums. Map keys and values must always have a value and cannot be empty ( null , nil , or None ). Enums are either strings or integers that can take only a fixed set of values. Enums with mixed value types are not supported. Strings are a sequence of bytes. Patterns are regular expressions. Integers are 64-bit numbers that can take both positive and negative values. Floats are 64-bit floating point numbers that can take both positive and negative values. Booleans are values of true or false and cannot take any other values. Planned future types Timestamps are nanosecond-scale timestamp values for a fixed time in UTC. They are stored and transported as integers, but may be unserialized from strings too. Dates are calendar dates without timezone information. Times are the time of a day denominated as hours, minutes, seconds, etc. on a nanosecond scale. Datetimes are date and times together in one field. Durations are nanosecond-scale timespan values. UUIDs are UUID-formatted strings. Sets are an unordered collection of items that can only contain unique items. Validation The typing system also contains more in-depth validation than just simple types: Strings Strings can have a minimum or maximum length, as well as validation against a regular expression. Ints, floats Number types can have a minimum and maximum value (inclusive). Booleans Boolean types can take a value of either true or false , but when unserializing from YAML or JSON formats, strings or int values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 are also accepted. Lists, maps Lists and maps can have constraints on the minimum or maximum number of items in them (inclusive). Objects Object fields can have several constraints: required_if has a list of other fields that, if set, make the current field required. required_if_not has a list of other fields that, if none are set, make the current field required. conflicts has a list of other fields that cannot be set together with the current field. OneOf When you need to create a list of multiple object types, or simply have an either-or choice between two object types, you can use the OneOf type. This field uses an already existing field of the underlying objects, or adds an extra field to the schema to distinguish between the different types. Translated to JSON, you might see something like this: { \"_type\" : \"Greeter\" , \"message\" : \"Hello world!\" } Metadata Object fields can also declare metadata that will help with creating user interfaces for the object. These fields are: name : A user-readable name for the field. description : A user-readable description for the field. It may contain newlines, but no other formatting is allowed. icon : SVG icon Intent inference For display purposes, the type system is designed so that it can infer the intent of the data. We wish to communicate the following intents: Graphs are x-y values of timestamps mapped to one or more values. Log lines are timestamps associated with text. Events are timestamps associated with other structured data. We explicitly document the following inference rules, which will probably change in the future. A map with keys of timestamps and values of integers or floats is rendered as a graph. A map with keys of timestamps and values of objects consisting only of integers and floats is rendered as a graph. A map with keys of timestamps and values of strings is considered a log line. A map with keys of timestamps and objects that don\u2019t match the rules above are considered an event. A map with keys of short strings and integer or float values is considered a pie chart. A list of objects consisting of a single timestamp and otherwise only integers and floats is rendered as a graph. A list of objects with a single timestamp and a single string are considered a log line. A list of objects with a single short string and a single integer or float is considered a pie chart. A list of objects consisting of no more than one timestamp and multiple other fields not matching the rules above is considered an event. If an object has a field called \u201ctitle\u201d, \u201cname\u201d, or \u201clabel\u201d, it will be used as a label for the current data set in a chart, or as a title for the wrapping box for the user interface elements.","title":"The typing system"},{"location":"arcaflow/concepts/typing/#the-arcaflow-type-system","text":"Arcaflow takes a departure from the classic run-and-pray approach of running workloads and validates workflows for validity before executing them. To do this, Arcaflow starts the plugins as needed before the workflow is run and queries them for their schema . This schema will contain information about what kind of input a plugin requests and what kind of outputs it can produce. A plugin can support multiple workflow steps and must provide information about the data types in its input and output for each step. A step can have exactly one input format, but may declare more than one output. The typesystem is inspired by JSON schema and OpenAPI , but it is more restrictive due to the need to efficiently serialize workloads over various formats.","title":"The Arcaflow type system"},{"location":"arcaflow/concepts/typing/#types","text":"The typing system supports the following data types. Objects are key-value pairs where the keys are always a fixed set of strings and values are of various types declared for each key. They are similar to classes in most programming languages. Fields in objects can be optional , which means they will have no value (commonly known as null , nil , or None ), or a default value. OneOf are a special type that is a union of multiple objects, distinguished by a special field called the discriminator. Lists are a sequence of values of the same type. The value type can be any of the other types described in this section. List items must always have a value and cannot be empty ( null , nil , or None ). Maps are key-value pairs that always have fixed types for both keys and values. Maps with mixed keys or values are not supported. Map keys can only be strings, integers, or enums. Map keys and values must always have a value and cannot be empty ( null , nil , or None ). Enums are either strings or integers that can take only a fixed set of values. Enums with mixed value types are not supported. Strings are a sequence of bytes. Patterns are regular expressions. Integers are 64-bit numbers that can take both positive and negative values. Floats are 64-bit floating point numbers that can take both positive and negative values. Booleans are values of true or false and cannot take any other values.","title":"Types"},{"location":"arcaflow/concepts/typing/#planned-future-types","text":"Timestamps are nanosecond-scale timestamp values for a fixed time in UTC. They are stored and transported as integers, but may be unserialized from strings too. Dates are calendar dates without timezone information. Times are the time of a day denominated as hours, minutes, seconds, etc. on a nanosecond scale. Datetimes are date and times together in one field. Durations are nanosecond-scale timespan values. UUIDs are UUID-formatted strings. Sets are an unordered collection of items that can only contain unique items.","title":"Planned future types"},{"location":"arcaflow/concepts/typing/#validation","text":"The typing system also contains more in-depth validation than just simple types:","title":"Validation"},{"location":"arcaflow/concepts/typing/#strings","text":"Strings can have a minimum or maximum length, as well as validation against a regular expression.","title":"Strings"},{"location":"arcaflow/concepts/typing/#ints-floats","text":"Number types can have a minimum and maximum value (inclusive).","title":"Ints, floats"},{"location":"arcaflow/concepts/typing/#booleans","text":"Boolean types can take a value of either true or false , but when unserializing from YAML or JSON formats, strings or int values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 are also accepted.","title":"Booleans"},{"location":"arcaflow/concepts/typing/#lists-maps","text":"Lists and maps can have constraints on the minimum or maximum number of items in them (inclusive).","title":"Lists, maps"},{"location":"arcaflow/concepts/typing/#objects","text":"Object fields can have several constraints: required_if has a list of other fields that, if set, make the current field required. required_if_not has a list of other fields that, if none are set, make the current field required. conflicts has a list of other fields that cannot be set together with the current field.","title":"Objects"},{"location":"arcaflow/concepts/typing/#oneof","text":"When you need to create a list of multiple object types, or simply have an either-or choice between two object types, you can use the OneOf type. This field uses an already existing field of the underlying objects, or adds an extra field to the schema to distinguish between the different types. Translated to JSON, you might see something like this: { \"_type\" : \"Greeter\" , \"message\" : \"Hello world!\" }","title":"OneOf"},{"location":"arcaflow/concepts/typing/#metadata","text":"Object fields can also declare metadata that will help with creating user interfaces for the object. These fields are: name : A user-readable name for the field. description : A user-readable description for the field. It may contain newlines, but no other formatting is allowed. icon : SVG icon","title":"Metadata"},{"location":"arcaflow/concepts/typing/#intent-inference","text":"For display purposes, the type system is designed so that it can infer the intent of the data. We wish to communicate the following intents: Graphs are x-y values of timestamps mapped to one or more values. Log lines are timestamps associated with text. Events are timestamps associated with other structured data. We explicitly document the following inference rules, which will probably change in the future. A map with keys of timestamps and values of integers or floats is rendered as a graph. A map with keys of timestamps and values of objects consisting only of integers and floats is rendered as a graph. A map with keys of timestamps and values of strings is considered a log line. A map with keys of timestamps and objects that don\u2019t match the rules above are considered an event. A map with keys of short strings and integer or float values is considered a pie chart. A list of objects consisting of a single timestamp and otherwise only integers and floats is rendered as a graph. A list of objects with a single timestamp and a single string are considered a log line. A list of objects with a single short string and a single integer or float is considered a pie chart. A list of objects consisting of no more than one timestamp and multiple other fields not matching the rules above is considered an event. If an object has a field called \u201ctitle\u201d, \u201cname\u201d, or \u201clabel\u201d, it will be used as a label for the current data set in a chart, or as a title for the wrapping box for the user interface elements.","title":"Intent inference"},{"location":"arcaflow/concepts/workflows/","text":"Arcaflow Workflows Steps Workflows are a way to describe a sequence or parallel execution of individual steps. The steps are provided exclusively by plugins. The simplest workflow looks like this: stateDiagram-v2 [*] --> Step Step --> [*] However, this is only true if the step only has one output. Most steps will at least have two possible outputs, for success and error states: stateDiagram-v2 [*] --> Step Step --> [*]: yes Step --> [*]: no Plugins can declare as many outputs as needed, with custom names. The workflow engine doesn\u2019t make a distinction based on the names, all outputs are treated equal for execution. However, a few names are treated special for display purposes only: stateDiagram-v2 [*] --> Step Step --> [*]: success Step --> [*]: warning Step --> [*]: error These three output names ( success , warning , and error ) will be colored accordingly in the user interfaces. Other names may be used, but will not be colored. An important rule is that one step must always end in exactly one output. No step must end without an output, and no step can end in more than one output. This provides a mechanism to direct the flow of the workflow execution. Plugins must also explicitly declare what parameters they expect as input for the step, and the data types of these and what parameters they will produce as output. For more detaisl about this see the Type system page . Interconnecting steps When two steps are connected, they will be executed after each other: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2 Step2 --> [*] Similarly, when two steps are not directly connected, they may be executed in parallel: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2 Step1 --> [*] Step2 --> [*] You can use the interconnection to direct the flow of step outputs: stateDiagram-v2 Step1: Step 1 Step2: Step 2 Step3: Step 3 [*] --> Step1 Step1 --> Step2: success Step1 --> Step3: error Step2 --> [*] Step3 --> [*] Passing data between steps When two steps are connected, you have the ability to pass data between them. Emblematically described: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2: input_1 = $.step1.output_1 Step2 --> [*] The data type of the input on Step 2 in this case must match the result of the expression. If the data type does not match, the workflow will not be executed. Undefined inputs Step inputs can either be required or optional. When a step input is required, it must be configured or the workflow will fail to execute. However, there are cases when the inputs cannot be determined from previous steps. In this case, the workflow start can be connected and the required inputs can be obtained from the user when running the workflow: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2: input_1 = $.start.option_1 Step1 --> Step2: input_2 = $.step1.output_1 Step2 --> [*] This is typically the case when credentials, such as database access, etc. are required. Outputs The output for each step is preserved for later inspection. However, the workflow can explicitly declare outputs. These outputs are usable in scripted environments as a direct output of the workflow: stateDiagram-v2 [*] --> Step Step --> [*]: output_1 = $.step1.output_1 Execution environment Workflow plugins will always run in containers. You can configure these containers to run locally, or you can connect a remote execution environment. At this time the two environments that are supported are Docker (or Podman using the Docker compatibility API) and Kubernetes. Later on we plan to add container execution via SSH. A local Docker (or Docker-like) environment is always required. The workflow engine needs this to obtain schema information from the plugins, as well as for mirroring plugins to a network-disconnected environment. Plugins must also make sure that they can execute in an unprivileged container, even when they later on need to be executed in a privileged environment. This is needed to obtain the schema before executing it in the target environment. You can configure any step to run in a remote environment as long as the remote environment can pull the container image. The engine provides the facilities to mirror the required plugins into a disconnected environment. Plugins must not make network calls during startup, and they should come with everything they need to run built in, unless their specific purpose is to install something on step execution. The engine will execute the plugin container image in a network-disconnected environment at startup to obtain its schema. If it fails to execute without internet, the workflow will not run. The execution environment has further parameters. For Docker, these options are specific to Docker, for Kubernetes they are specific to Kubernetes. For Kubernetes, you can also specify constraints on where the step is executed. Flow control The workflow contains several flow control operations. These flow control operations are not implemented by plugins, but are part of the workflow engine itself. Abort The abort flow control is a quick way to exit out of a workflow. This is useful when entering a terminal error state and the workflow output data would be useless anyway. stateDiagram-v2 [*] --> Step1 Step1 --> Abort: Output 1 Step1 --> Step2: Output 2 Step2 --> [*] Do-while A do-while block will execute the steps in it as long as a certain condition is met. The condition is derived from the output of the step or steps executed inside the loop: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: output_1_condition=$.step1.output_1.finished == false } DoWhile --> [*] If the step declares multiple outputs, multiple conditions are possible. The do-while block will also have multiple outputs: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: Output 1 condition Step1 --> [*]: Output 2 condition } DoWhile --> [*]: Output 1 DoWhile --> [*]: Output 2 You may decide to only allow exit from a loop if one of the two outputs is satisfied: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> Step1: Output 1 Step1 --> [*]: Output 2 } DoWhile --> [*]: Output 1 If you leave one of the outputs disconnected, that loop will automatically lead to the loop being repeated. Warning Do not leave a failure condition unconnected as this may result in an endless loop! Use the Abort flow control operation instead! Condition A condition is a flow control operation that redirects the flow one way or another based on an expression. You can also create multiple branches to create a switch-case effect. stateDiagram-v2 state if_state <<choice>> Step1: Step 1 [*] --> Step1 Step1 --> if_state Step2: Step 2 Step3: Step 3 if_state --> Step2: $.step1.output_1 == true if_state --> Step3: $.step1.output_1 == false Multiply The multiply flow control operation is useful when you need to dynamically execute sub-workflows in parallel based on an input condition. You can, for example, use this to run a workflow step on multiple or all Kubernetes nodes. stateDiagram-v2 Lookup: Lookup Kubernetes hosts [*] --> Lookup Lookup --> Multiply state Multiply { [*] --> Stresstest Stresstest --> [*] } Multiply --> [*] The output of a Multiply operation will be a map, keyed with a string that is configured from the input. Tip You can think of a Multiply step like a for-each loop, but the steps being executed in parallel. Synchronize The synchronize step attempts to synchronize the execution of subsequent steps for a specified key. The key must be a constant and cannot be obtained from an input expression. stateDiagram-v2 [*] --> Step1 [*] --> Step2 Synchronize1: Synchronize (key=a) Synchronize2: Synchronize (key=a) Step1 --> Synchronize1 Step2 --> Synchronize2 Synchronize1 --> Step3 Synchronize2 --> Step4 Step3 --> [*] Step4 --> [*]","title":"Workflows"},{"location":"arcaflow/concepts/workflows/#arcaflow-workflows","text":"","title":"Arcaflow Workflows"},{"location":"arcaflow/concepts/workflows/#steps","text":"Workflows are a way to describe a sequence or parallel execution of individual steps. The steps are provided exclusively by plugins. The simplest workflow looks like this: stateDiagram-v2 [*] --> Step Step --> [*] However, this is only true if the step only has one output. Most steps will at least have two possible outputs, for success and error states: stateDiagram-v2 [*] --> Step Step --> [*]: yes Step --> [*]: no Plugins can declare as many outputs as needed, with custom names. The workflow engine doesn\u2019t make a distinction based on the names, all outputs are treated equal for execution. However, a few names are treated special for display purposes only: stateDiagram-v2 [*] --> Step Step --> [*]: success Step --> [*]: warning Step --> [*]: error These three output names ( success , warning , and error ) will be colored accordingly in the user interfaces. Other names may be used, but will not be colored. An important rule is that one step must always end in exactly one output. No step must end without an output, and no step can end in more than one output. This provides a mechanism to direct the flow of the workflow execution. Plugins must also explicitly declare what parameters they expect as input for the step, and the data types of these and what parameters they will produce as output. For more detaisl about this see the Type system page .","title":"Steps"},{"location":"arcaflow/concepts/workflows/#interconnecting-steps","text":"When two steps are connected, they will be executed after each other: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2 Step2 --> [*] Similarly, when two steps are not directly connected, they may be executed in parallel: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2 Step1 --> [*] Step2 --> [*] You can use the interconnection to direct the flow of step outputs: stateDiagram-v2 Step1: Step 1 Step2: Step 2 Step3: Step 3 [*] --> Step1 Step1 --> Step2: success Step1 --> Step3: error Step2 --> [*] Step3 --> [*]","title":"Interconnecting steps"},{"location":"arcaflow/concepts/workflows/#passing-data-between-steps","text":"When two steps are connected, you have the ability to pass data between them. Emblematically described: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 Step1 --> Step2: input_1 = $.step1.output_1 Step2 --> [*] The data type of the input on Step 2 in this case must match the result of the expression. If the data type does not match, the workflow will not be executed.","title":"Passing data between steps"},{"location":"arcaflow/concepts/workflows/#undefined-inputs","text":"Step inputs can either be required or optional. When a step input is required, it must be configured or the workflow will fail to execute. However, there are cases when the inputs cannot be determined from previous steps. In this case, the workflow start can be connected and the required inputs can be obtained from the user when running the workflow: stateDiagram-v2 Step1: Step 1 Step2: Step 2 [*] --> Step1 [*] --> Step2: input_1 = $.start.option_1 Step1 --> Step2: input_2 = $.step1.output_1 Step2 --> [*] This is typically the case when credentials, such as database access, etc. are required.","title":"Undefined inputs"},{"location":"arcaflow/concepts/workflows/#outputs","text":"The output for each step is preserved for later inspection. However, the workflow can explicitly declare outputs. These outputs are usable in scripted environments as a direct output of the workflow: stateDiagram-v2 [*] --> Step Step --> [*]: output_1 = $.step1.output_1","title":"Outputs"},{"location":"arcaflow/concepts/workflows/#execution-environment","text":"Workflow plugins will always run in containers. You can configure these containers to run locally, or you can connect a remote execution environment. At this time the two environments that are supported are Docker (or Podman using the Docker compatibility API) and Kubernetes. Later on we plan to add container execution via SSH. A local Docker (or Docker-like) environment is always required. The workflow engine needs this to obtain schema information from the plugins, as well as for mirroring plugins to a network-disconnected environment. Plugins must also make sure that they can execute in an unprivileged container, even when they later on need to be executed in a privileged environment. This is needed to obtain the schema before executing it in the target environment. You can configure any step to run in a remote environment as long as the remote environment can pull the container image. The engine provides the facilities to mirror the required plugins into a disconnected environment. Plugins must not make network calls during startup, and they should come with everything they need to run built in, unless their specific purpose is to install something on step execution. The engine will execute the plugin container image in a network-disconnected environment at startup to obtain its schema. If it fails to execute without internet, the workflow will not run. The execution environment has further parameters. For Docker, these options are specific to Docker, for Kubernetes they are specific to Kubernetes. For Kubernetes, you can also specify constraints on where the step is executed.","title":"Execution environment"},{"location":"arcaflow/concepts/workflows/#flow-control","text":"The workflow contains several flow control operations. These flow control operations are not implemented by plugins, but are part of the workflow engine itself.","title":"Flow control"},{"location":"arcaflow/concepts/workflows/#abort","text":"The abort flow control is a quick way to exit out of a workflow. This is useful when entering a terminal error state and the workflow output data would be useless anyway. stateDiagram-v2 [*] --> Step1 Step1 --> Abort: Output 1 Step1 --> Step2: Output 2 Step2 --> [*]","title":"Abort"},{"location":"arcaflow/concepts/workflows/#do-while","text":"A do-while block will execute the steps in it as long as a certain condition is met. The condition is derived from the output of the step or steps executed inside the loop: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: output_1_condition=$.step1.output_1.finished == false } DoWhile --> [*] If the step declares multiple outputs, multiple conditions are possible. The do-while block will also have multiple outputs: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> [*]: Output 1 condition Step1 --> [*]: Output 2 condition } DoWhile --> [*]: Output 1 DoWhile --> [*]: Output 2 You may decide to only allow exit from a loop if one of the two outputs is satisfied: stateDiagram-v2 [*] --> DoWhile state DoWhile { [*] --> Step1 Step1 --> Step1: Output 1 Step1 --> [*]: Output 2 } DoWhile --> [*]: Output 1 If you leave one of the outputs disconnected, that loop will automatically lead to the loop being repeated. Warning Do not leave a failure condition unconnected as this may result in an endless loop! Use the Abort flow control operation instead!","title":"Do-while"},{"location":"arcaflow/concepts/workflows/#condition","text":"A condition is a flow control operation that redirects the flow one way or another based on an expression. You can also create multiple branches to create a switch-case effect. stateDiagram-v2 state if_state <<choice>> Step1: Step 1 [*] --> Step1 Step1 --> if_state Step2: Step 2 Step3: Step 3 if_state --> Step2: $.step1.output_1 == true if_state --> Step3: $.step1.output_1 == false","title":"Condition"},{"location":"arcaflow/concepts/workflows/#multiply","text":"The multiply flow control operation is useful when you need to dynamically execute sub-workflows in parallel based on an input condition. You can, for example, use this to run a workflow step on multiple or all Kubernetes nodes. stateDiagram-v2 Lookup: Lookup Kubernetes hosts [*] --> Lookup Lookup --> Multiply state Multiply { [*] --> Stresstest Stresstest --> [*] } Multiply --> [*] The output of a Multiply operation will be a map, keyed with a string that is configured from the input. Tip You can think of a Multiply step like a for-each loop, but the steps being executed in parallel.","title":"Multiply"},{"location":"arcaflow/concepts/workflows/#synchronize","text":"The synchronize step attempts to synchronize the execution of subsequent steps for a specified key. The key must be a constant and cannot be obtained from an input expression. stateDiagram-v2 [*] --> Step1 [*] --> Step2 Synchronize1: Synchronize (key=a) Synchronize2: Synchronize (key=a) Step1 --> Synchronize1 Step2 --> Synchronize2 Synchronize1 --> Step3 Synchronize2 --> Step4 Step3 --> [*] Step4 --> [*]","title":"Synchronize"},{"location":"arcaflow/creating-plugins/python/","text":"Creating plugins with Python If you want to create an Arcaflow plugin in Python, you will need three things: A container engine that can build images Python 3.9+ ( PyPy is supported) The Python SDK for Arcaflow plugins The easiest way is to start from the template repository for Python plugins , but starting from scratch is also fully supported. Before you start please familiarize yourself with the Arcaflow type system . Setting up your environment First, you will have to set up your environment. From the template repository Using pip Using Poetry Fork, then clone the template repository Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Create an empty folder. Create a requirements.txt with the following content: arcaflow-plugin-sdk Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Copy the example plugin , example config and the tests to your directory. Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Assuming you have Poetry installed, run the following command: poetry new your-plugin Then change the current directory to your-plugin . Figure out what the right command to call your Python version is: which python3.10 which python3.9 which python3 which python Make sure you have at least Python 3.9. Set Poetry to Python 3.9: poetry env use /path/to/your/python3.9 Check that your pyproject.toml file has the following lines: [tool.poetry.dependencies] python = \"^3.9\" Add the SDK as a dependency: poetry add arcaflow-plugin-sdk Copy the example plugin , example config and the tests to your directory. Activate the venv: poetry shell Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Now you are ready to start hacking away at your plugin! Creating your plugin the easy way A plugin is nothing but a list of functions with type-annotated parameters and decorators. For example, let\u2019s create a function: def pod_scenario ( input_parameter ): # Do pod scenario magic here However, this SDK uses Python type hints and decorators to automatically generate the schema required for Arcaflow. Alternatively, you can also build a schema by hand . The current section describes the automated way, the section below describes the manual way. Input parameters Your step function must take exactly one input parameter. This parameter must be a dataclass . For example: import dataclasses import re @dataclasses . dataclass class PodScenarioParams : namespace_pattern : re . Pattern = re . compile ( \".*\" ) pod_name_pattern : re . Pattern = re . compile ( \".*\" ) As you can see, our dataclass has two fields, each of which is a re.Pattern . This SDK automatically reads the types of the fields to construct the schema. See the Types section below for supported type patterns. Output parameters Now that you have your input parameter class, you must create one or more output classes in a similar fashion: import dataclasses import typing @dataclasses . dataclass class Pod : namespace : str name : str @dataclasses . dataclass class PodScenarioResults : pods_killed : typing . List [ Pod ] As you can see, your input may incorporate other classes, which themselves have to be dataclasses. Read on for more information on types . Creating a step function Now that we have both our input and output(s), let\u2019s go back to our initial pod_scenario function. Here we need to add a decorator to tell the SDK about metadata, and more importantly, what the return types are. (This is needed because Python does not support reading return types to an adequate level.) from arcaflow_plugin_sdk import plugin @plugin . step ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kill one or more pods matching the criteria\" , outputs = { \"success\" : PodScenarioResults , \"error\" : PodScenarioError }, ) def pod_scenario ( params : PodScenarioParams ): # Fail for now return \"error\" , PodScenarioError ( \"Not implemented\" ) As you can see, apart from the metadata, we also declare the type of the parameter object so the SDK can read it. Let\u2019s go through the @plugin.step decorator parameters one by one: id indicates the identifier of this step. This must be globally unique name indicates a human-readable name for this step description indicates a longer description for this step outputs indicates which possible outputs the step can have, with their output identifiers as keys The function must return the output identifier, along with the output object. Running the plugin Finally, we need to call plugin.run() in order to actually run the plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( plugin . build_schema ( # Pass one or more scenario functions here pod_scenario , ))) You can now call your plugin using ./yourscript.py -f path-to-parameters.yaml . If you have defined more than one step, you also need to pass the -s step-id parameter. Keep in mind, you should always test your plugin. See Testing your plugin below for details. Types The SDK supports a wide range of types. Let\u2019s start with the basics: str int float bool Enums re.Pattern typing.List[othertype] (you must specify the type for the contents of the list) typing.Dict[keytype, valuetype] (you must specify the type for the keys and values) Any dataclass Optional parameters You can also declare any parameter as optional like this: @dataclasses . dataclass class MyClass : param : typing . Optional [ int ] = None Note that adding typing.Optional is not enough, you must specify the default value. Union types Union types are supported as long as all members are dataclasses. For example: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Union [ A , B ]] In the underlying transport a field name _type will be added to act as a serialization discriminator. You can also customize the discriminator field: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ A , B ], annotations . discriminator ( \"foo\" ) ] ] If you intend to use a non-string descriminator field, or you want to manually specify the discriminator value, you can do so by adding a discriminator_value annotation: @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ typing . Annotated [ A , annotations . discriminator_value ( \"first\" )], typing . Annotated [ B , annotations . discriminator_value ( \"second\" )] ], annotations . discriminator ( \"foo\" ) ] ] Tip You can add the discriminator field to your underlying dataclasses, but when present, their schema must match exactly . Validation You can also validate the values by using typing.Annotated , such as this: class MyClass : param : typing . Annotated [ int , validation . min ( 5 )] This will create a minimum-value validation for the parameter of 5. The following annotations are supported for validation: validation.min() for strings, ints, floats, lists, and maps validation.max() for strings, ints, floats, lists, and maps validation.pattern() for strings validation.required_if() for any field on an object validation.required_if_not() for any field on an object validation.conflicts() for any field on an object Metadata You can add metadata to your schema by using the field() parameter for dataclasses, for example: @dataclasses . dataclass class MyClass : param : str = field ( metadata = { \"id\" : \"my-param\" , \"name\" : \"Parameter 1\" , \"description\" : \"This is a parameter\" }) Creating your plugin the hard way For performance reasons, or for the purposes of separation of concerns, you may want to create a schema by hand. This section walks you through declaring a schema by hand and then using it to call a function. Keep in mind, the SDK still primarily operates with dataclasses to transport structured data. We start by defining a schema: from arcaflow_plugin_sdk import schema from typing import Dict steps : Dict [ str , schema . StepSchema ] s = schema . Schema ( steps , ) The steps parameter here must be a dict, where the key is the step ID and the value is the step schema. So, let\u2019s create a step schema: from arcaflow_plugin_sdk import schema step_schema = schema . StepSchema ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kills pods\" , input = input_schema , outputs = outputs , handler = my_handler_func ) Let\u2019s go in order: The input must be a schema of the type schema.ObjectType . This describes the single parameter that will be passed to my_handler_func . The outputs describe a Dict[str, schema.ObjectType] , where the key is the ID for the returned output type, while the value describes the output schema. The handler function takes one parameter, the object described in input and must return a tuple of a string and the output object. Here the ID uniquely identifies which output is intended, for example success and error , while the second parameter in the tuple must match the outputs declaration. That\u2019s it! Now all that\u2019s left is to define the ObjectType and any subobjects. ObjectType The ObjectType is intended as a backing type for dataclasses . For example: t = schema . ObjectType ( TestClass , { \"a\" : schema . Field ( type = schema . StringType (), required = True , ), \"b\" : schema . Field ( type = schema . IntType (), required = True , ) } ) The fields support the following parameters: type : underlying type schema for the field (required) name : name for the current field description : description for the current field required : marks the field as required required_if : a list of other fields that, if filled, will also cause the current field to be required required_if_not : a list of other fields that, if not set, will cause the current field to be required conflicts : a list of other fields that cannot be set together with the current field AnyOfType The AnyOfType allows you to create a type that is a combination of other ObjectTypes. When a value is deserialized, a special discriminator field is consulted to figure out which type is actually being sent. This discriminator field may be present in the underlying type. If it is, the type must match the declaration in the AnyOfType. For example: @dataclasses . dataclass class OneOfData1 : type : str a : str @dataclasses . dataclass class OneOfData2 : b : int s = schema . OneOfType ( # Discriminator field \"type\" , # Discriminator field type schema . StringType (), { # Option 1 \"a\" : schema . ObjectType ( OneOfData1 , { # Here the discriminator field is also present in the underlying type \"type\" : schema . Field ( schema . StringType (), ), \"a\" : schema . Field ( schema . StringType () ) } ), # Option 2 \"b\" : schema . ObjectType ( OneOfData2 , { \"b\" : schema . Field ( schema . IntType () ) } ) } ) serialized_data = s . serialize ( OneOfData1 ( \"a\" , \"Hello world!\" )) pprint . pprint ( serialized_data ) StringType String types indicate that the underlying type is a string. t = schema . StringType () The string type supports the following parameters: min_length : minimum length for the string (inclusive) max_length : maximum length for the string (inclusive) pattern : regular expression the string must match PatternType The pattern type indicates that the field must contain a regular expression. It will be decoded as re.Pattern . t = schema . PatternType () The pattern type has no parameters. IntType The int type indicates that the underlying type is an integer. t = schema . IntType () The int type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive). FloatType The float type indicates that the underlying type is a floating point number. t = schema . FloatType () The float type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive). BoolType The bool type indicates that the underlying value is a boolean. When unserializing, this type also supports string and integer values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 . EnumType The enum type creates a type from an existing enum: class MyEnum ( Enum ): A = \"a\" B = \"b\" t = schema . EnumType ( MyEnum ) The enum type has no further parameters. ListType The list type describes a list of items. The item type must be described: t = schema . ListType ( schema . StringType () ) The list type supports the following extra parameters: min : The minimum number of items in the list (inclusive) max : The maximum number of items in the list (inclusive) MapType The map type describes a key-value type (dict). You must specify both the key and the value type: t = schema . MapType ( schema . StringType (), schema . StringType () ) The map type supports the following extra parameters: min : The minimum number of items in the map (inclusive) max : The maximum number of items in the map (inclusive) Running the plugin If you create the schema by hand, you can add the following code to your plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( your_schema )) You can then run your plugin as described before. Testing your plugin You should always make sure you have enough test coverage to prevent your plugin from breaking. To help you with testing, this SDK provides some tools for testing: Serialization tests for your input and output to make sure your classes can be serialized for transport Functional tests that call your plugin and make sure it works correctly Writing a serialization test You can use any test framework you like for your serialization test, we\u2019ll demonstrate with unittest as it is included directly in Python. The key to this test is to call plugin.test_object_serialization() with an instance of your dataclass that you want to test: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): self . assertTrue ( plugin . test_object_serialization ( example_plugin . PodScenarioResults ( [ example_plugin . Pod ( namespace = \"default\" , name = \"nginx-asdf\" ) ] ) )) Remember, you need to call this function with an instance containing actual data, not just the class name. The test function will first serialize, then unserialize your data and check if it\u2019s the same. If you want to use a manually created schema, you can do so, too: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): plugin . test_object_serialization ( example_plugin . PodScenarioResults ( #... ), schema . ObjectType ( #... ) ) Functional tests Functional tests don\u2019t have anything special about them. You can directly call your code with your dataclasses as parameters, and check the return. This works best on auto-generated schemas with the @plugin.step decorator. See below for manually created schemas. class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): input = example_plugin . PodScenarioParams () output_id , output_data = example_plugin . pod_scenario ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) ) If you created your schema manually, the best way to write your tests is to include the schema in your test. This will automatically validate both the input and the output, making sure they conform to your schema. For example: class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): step_schema = schema . StepSchema ( #... handler = example_plugin . pod_scenario , ) input = example_plugin . PodScenarioParams () output_id , output_data = step_schema ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) ) Embedding your plugin Instead of using your plugin as a standalone tool or in conjunction with Arcaflow, you can also embed your plugin into your existing Python application. To do that you simply build a schema using one of the methods described above and then call the schema yourself. You can pass raw data as an input, and you\u2019ll get the benefit of schema validation. # Build your schema using the schema builder from above with the step functions passed. schema = plugin . build_schema ( pod_scenario ) # Which step we want to execute step_id = \"pod\" # Input parameters. Note, these must be a dict, not a dataclass step_params = { \"pod_name_pattern\" : \".*\" , \"pod_namespace_pattern\" : \".*\" , } # Execute the step output_id , output_data = schema ( step_id , step_params ) # Print which kind of result we have pprint . pprint ( output_id ) # Print the result data pprint . pprint ( output_data ) However, the example above requires you to provide the data as a dict , not a dataclass , and it will also return a dict as an output object. Sometimes, you may want to use a partial approach, where you only use part of the SDK. In this case, you can change your code to run any of the following functions, in order: serialization.load_from_file() to load a YAML or JSON file into a dict yourschema.unserialize_input() to turn a dict into a dataclass needed for your steps yourschema.call_step() to run a step with the unserialized dataclass yourschema.serialize_output() to turn the output dataclass into a dict FAQ How can I add a field with dashes, such as my-field ? Dataclasses don\u2019t support dashes in parameters. You can work around this by defining the id metadata field: @dataclasses . dataclass class MyData : my_field : str = dataclasses . field ( metadata = { \"id\" : \"my-field\" }) How can I write a dataclass from a schema to a YAML or JSON file? You can extend Pythons JSON encoder to support dataclasses. If that doesn\u2019t suit your needs, you can use this SDK to convert the dataclasses to their basic representations and then write that to your JSON or YAML file. First, add this outside of your step: my_object_schema = plugin . build_object_schema ( YourDataclass ) Inside your step function you can then dump the data from your input def your_step ( params : YourParams ) yaml_contents = yaml . dump ( my_object_schema . serialize ( params . some_param )) How can I easily load a list from a YAML or JSON into a list of dataclasses? This requires a bit of trickery. First, we build a schema from the dataclass representing the row or entry in the list: my_row_schema = plugin . build_object_schema ( MyRow ) Then you can create a list schema: my_list_schema = schema . ListType ( my_row_schema ) You can now unserialize a list obtained from the YAML or JSON file: my_data = my_list_schema . unserialize ( json . loads ( ... ))","title":"Python"},{"location":"arcaflow/creating-plugins/python/#creating-plugins-with-python","text":"If you want to create an Arcaflow plugin in Python, you will need three things: A container engine that can build images Python 3.9+ ( PyPy is supported) The Python SDK for Arcaflow plugins The easiest way is to start from the template repository for Python plugins , but starting from scratch is also fully supported. Before you start please familiarize yourself with the Arcaflow type system .","title":"Creating plugins with Python"},{"location":"arcaflow/creating-plugins/python/#setting-up-your-environment","text":"First, you will have to set up your environment. From the template repository Using pip Using Poetry Fork, then clone the template repository Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Create an empty folder. Create a requirements.txt with the following content: arcaflow-plugin-sdk Figure out what the right command to call your Python version is: python3.10 --version python3.9 --version python3 --version python --version Make sure you have at least Python 3.9. Create a virtualenv in your project directory using the following command, replacing your Python call: python -m venv venv Activate the venv: source venv/bin/activate Install the dependencies: pip install -r requirements.txt Copy the example plugin , example config and the tests to your directory. Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Assuming you have Poetry installed, run the following command: poetry new your-plugin Then change the current directory to your-plugin . Figure out what the right command to call your Python version is: which python3.10 which python3.9 which python3 which python Make sure you have at least Python 3.9. Set Poetry to Python 3.9: poetry env use /path/to/your/python3.9 Check that your pyproject.toml file has the following lines: [tool.poetry.dependencies] python = \"^3.9\" Add the SDK as a dependency: poetry add arcaflow-plugin-sdk Copy the example plugin , example config and the tests to your directory. Activate the venv: poetry shell Run the test plugin: ./example_plugin.py -f example.yaml Run the unit tests: ./test_example_plugin.py Generate a JSON schema: ./example_plugin.py --json-schema input >example.schema.json If you are using the YAML plugin for VSCode , add the following line to the top of your config file for code completion: # yaml-language-server: $schema=example.schema.json Copy and customize the Dockerfile from the example repository. Set up your CI/CD system as you see fit. Now you are ready to start hacking away at your plugin!","title":"Setting up your environment"},{"location":"arcaflow/creating-plugins/python/#creating-your-plugin-the-easy-way","text":"A plugin is nothing but a list of functions with type-annotated parameters and decorators. For example, let\u2019s create a function: def pod_scenario ( input_parameter ): # Do pod scenario magic here However, this SDK uses Python type hints and decorators to automatically generate the schema required for Arcaflow. Alternatively, you can also build a schema by hand . The current section describes the automated way, the section below describes the manual way.","title":"Creating your plugin the easy way"},{"location":"arcaflow/creating-plugins/python/#input-parameters","text":"Your step function must take exactly one input parameter. This parameter must be a dataclass . For example: import dataclasses import re @dataclasses . dataclass class PodScenarioParams : namespace_pattern : re . Pattern = re . compile ( \".*\" ) pod_name_pattern : re . Pattern = re . compile ( \".*\" ) As you can see, our dataclass has two fields, each of which is a re.Pattern . This SDK automatically reads the types of the fields to construct the schema. See the Types section below for supported type patterns.","title":"Input parameters"},{"location":"arcaflow/creating-plugins/python/#output-parameters","text":"Now that you have your input parameter class, you must create one or more output classes in a similar fashion: import dataclasses import typing @dataclasses . dataclass class Pod : namespace : str name : str @dataclasses . dataclass class PodScenarioResults : pods_killed : typing . List [ Pod ] As you can see, your input may incorporate other classes, which themselves have to be dataclasses. Read on for more information on types .","title":"Output parameters"},{"location":"arcaflow/creating-plugins/python/#creating-a-step-function","text":"Now that we have both our input and output(s), let\u2019s go back to our initial pod_scenario function. Here we need to add a decorator to tell the SDK about metadata, and more importantly, what the return types are. (This is needed because Python does not support reading return types to an adequate level.) from arcaflow_plugin_sdk import plugin @plugin . step ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kill one or more pods matching the criteria\" , outputs = { \"success\" : PodScenarioResults , \"error\" : PodScenarioError }, ) def pod_scenario ( params : PodScenarioParams ): # Fail for now return \"error\" , PodScenarioError ( \"Not implemented\" ) As you can see, apart from the metadata, we also declare the type of the parameter object so the SDK can read it. Let\u2019s go through the @plugin.step decorator parameters one by one: id indicates the identifier of this step. This must be globally unique name indicates a human-readable name for this step description indicates a longer description for this step outputs indicates which possible outputs the step can have, with their output identifiers as keys The function must return the output identifier, along with the output object.","title":"Creating a step function"},{"location":"arcaflow/creating-plugins/python/#running-the-plugin","text":"Finally, we need to call plugin.run() in order to actually run the plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( plugin . build_schema ( # Pass one or more scenario functions here pod_scenario , ))) You can now call your plugin using ./yourscript.py -f path-to-parameters.yaml . If you have defined more than one step, you also need to pass the -s step-id parameter. Keep in mind, you should always test your plugin. See Testing your plugin below for details.","title":"Running the plugin"},{"location":"arcaflow/creating-plugins/python/#types","text":"The SDK supports a wide range of types. Let\u2019s start with the basics: str int float bool Enums re.Pattern typing.List[othertype] (you must specify the type for the contents of the list) typing.Dict[keytype, valuetype] (you must specify the type for the keys and values) Any dataclass","title":"Types"},{"location":"arcaflow/creating-plugins/python/#optional-parameters","text":"You can also declare any parameter as optional like this: @dataclasses . dataclass class MyClass : param : typing . Optional [ int ] = None Note that adding typing.Optional is not enough, you must specify the default value.","title":"Optional parameters"},{"location":"arcaflow/creating-plugins/python/#union-types","text":"Union types are supported as long as all members are dataclasses. For example: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Union [ A , B ]] In the underlying transport a field name _type will be added to act as a serialization discriminator. You can also customize the discriminator field: @dataclasses . dataclass class A : a : str @dataclasses . dataclass class B : b : str @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ A , B ], annotations . discriminator ( \"foo\" ) ] ] If you intend to use a non-string descriminator field, or you want to manually specify the discriminator value, you can do so by adding a discriminator_value annotation: @dataclasses . dataclass class MyParams : items : typing . List [ typing . Annotated [ typing . Union [ typing . Annotated [ A , annotations . discriminator_value ( \"first\" )], typing . Annotated [ B , annotations . discriminator_value ( \"second\" )] ], annotations . discriminator ( \"foo\" ) ] ] Tip You can add the discriminator field to your underlying dataclasses, but when present, their schema must match exactly .","title":"Union types"},{"location":"arcaflow/creating-plugins/python/#validation","text":"You can also validate the values by using typing.Annotated , such as this: class MyClass : param : typing . Annotated [ int , validation . min ( 5 )] This will create a minimum-value validation for the parameter of 5. The following annotations are supported for validation: validation.min() for strings, ints, floats, lists, and maps validation.max() for strings, ints, floats, lists, and maps validation.pattern() for strings validation.required_if() for any field on an object validation.required_if_not() for any field on an object validation.conflicts() for any field on an object","title":"Validation"},{"location":"arcaflow/creating-plugins/python/#metadata","text":"You can add metadata to your schema by using the field() parameter for dataclasses, for example: @dataclasses . dataclass class MyClass : param : str = field ( metadata = { \"id\" : \"my-param\" , \"name\" : \"Parameter 1\" , \"description\" : \"This is a parameter\" })","title":"Metadata"},{"location":"arcaflow/creating-plugins/python/#creating-your-plugin-the-hard-way","text":"For performance reasons, or for the purposes of separation of concerns, you may want to create a schema by hand. This section walks you through declaring a schema by hand and then using it to call a function. Keep in mind, the SDK still primarily operates with dataclasses to transport structured data. We start by defining a schema: from arcaflow_plugin_sdk import schema from typing import Dict steps : Dict [ str , schema . StepSchema ] s = schema . Schema ( steps , ) The steps parameter here must be a dict, where the key is the step ID and the value is the step schema. So, let\u2019s create a step schema: from arcaflow_plugin_sdk import schema step_schema = schema . StepSchema ( id = \"pod\" , name = \"Pod scenario\" , description = \"Kills pods\" , input = input_schema , outputs = outputs , handler = my_handler_func ) Let\u2019s go in order: The input must be a schema of the type schema.ObjectType . This describes the single parameter that will be passed to my_handler_func . The outputs describe a Dict[str, schema.ObjectType] , where the key is the ID for the returned output type, while the value describes the output schema. The handler function takes one parameter, the object described in input and must return a tuple of a string and the output object. Here the ID uniquely identifies which output is intended, for example success and error , while the second parameter in the tuple must match the outputs declaration. That\u2019s it! Now all that\u2019s left is to define the ObjectType and any subobjects.","title":"Creating your plugin the hard way"},{"location":"arcaflow/creating-plugins/python/#objecttype","text":"The ObjectType is intended as a backing type for dataclasses . For example: t = schema . ObjectType ( TestClass , { \"a\" : schema . Field ( type = schema . StringType (), required = True , ), \"b\" : schema . Field ( type = schema . IntType (), required = True , ) } ) The fields support the following parameters: type : underlying type schema for the field (required) name : name for the current field description : description for the current field required : marks the field as required required_if : a list of other fields that, if filled, will also cause the current field to be required required_if_not : a list of other fields that, if not set, will cause the current field to be required conflicts : a list of other fields that cannot be set together with the current field","title":"ObjectType"},{"location":"arcaflow/creating-plugins/python/#anyoftype","text":"The AnyOfType allows you to create a type that is a combination of other ObjectTypes. When a value is deserialized, a special discriminator field is consulted to figure out which type is actually being sent. This discriminator field may be present in the underlying type. If it is, the type must match the declaration in the AnyOfType. For example: @dataclasses . dataclass class OneOfData1 : type : str a : str @dataclasses . dataclass class OneOfData2 : b : int s = schema . OneOfType ( # Discriminator field \"type\" , # Discriminator field type schema . StringType (), { # Option 1 \"a\" : schema . ObjectType ( OneOfData1 , { # Here the discriminator field is also present in the underlying type \"type\" : schema . Field ( schema . StringType (), ), \"a\" : schema . Field ( schema . StringType () ) } ), # Option 2 \"b\" : schema . ObjectType ( OneOfData2 , { \"b\" : schema . Field ( schema . IntType () ) } ) } ) serialized_data = s . serialize ( OneOfData1 ( \"a\" , \"Hello world!\" )) pprint . pprint ( serialized_data )","title":"AnyOfType"},{"location":"arcaflow/creating-plugins/python/#stringtype","text":"String types indicate that the underlying type is a string. t = schema . StringType () The string type supports the following parameters: min_length : minimum length for the string (inclusive) max_length : maximum length for the string (inclusive) pattern : regular expression the string must match","title":"StringType"},{"location":"arcaflow/creating-plugins/python/#patterntype","text":"The pattern type indicates that the field must contain a regular expression. It will be decoded as re.Pattern . t = schema . PatternType () The pattern type has no parameters.","title":"PatternType"},{"location":"arcaflow/creating-plugins/python/#inttype","text":"The int type indicates that the underlying type is an integer. t = schema . IntType () The int type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive).","title":"IntType"},{"location":"arcaflow/creating-plugins/python/#floattype","text":"The float type indicates that the underlying type is a floating point number. t = schema . FloatType () The float type supports the following parameters: min : minimum value for the number (inclusive). max : minimum value for the number (inclusive).","title":"FloatType"},{"location":"arcaflow/creating-plugins/python/#booltype","text":"The bool type indicates that the underlying value is a boolean. When unserializing, this type also supports string and integer values of true , yes , on , enable , enabled , 1 , false , no , off , disable , disabled or 0 .","title":"BoolType"},{"location":"arcaflow/creating-plugins/python/#enumtype","text":"The enum type creates a type from an existing enum: class MyEnum ( Enum ): A = \"a\" B = \"b\" t = schema . EnumType ( MyEnum ) The enum type has no further parameters.","title":"EnumType"},{"location":"arcaflow/creating-plugins/python/#listtype","text":"The list type describes a list of items. The item type must be described: t = schema . ListType ( schema . StringType () ) The list type supports the following extra parameters: min : The minimum number of items in the list (inclusive) max : The maximum number of items in the list (inclusive)","title":"ListType"},{"location":"arcaflow/creating-plugins/python/#maptype","text":"The map type describes a key-value type (dict). You must specify both the key and the value type: t = schema . MapType ( schema . StringType (), schema . StringType () ) The map type supports the following extra parameters: min : The minimum number of items in the map (inclusive) max : The maximum number of items in the map (inclusive)","title":"MapType"},{"location":"arcaflow/creating-plugins/python/#running-the-plugin_1","text":"If you create the schema by hand, you can add the following code to your plugin: if __name__ == \"__main__\" : sys . exit ( plugin . run ( your_schema )) You can then run your plugin as described before.","title":"Running the plugin"},{"location":"arcaflow/creating-plugins/python/#testing-your-plugin","text":"You should always make sure you have enough test coverage to prevent your plugin from breaking. To help you with testing, this SDK provides some tools for testing: Serialization tests for your input and output to make sure your classes can be serialized for transport Functional tests that call your plugin and make sure it works correctly","title":"Testing your plugin"},{"location":"arcaflow/creating-plugins/python/#writing-a-serialization-test","text":"You can use any test framework you like for your serialization test, we\u2019ll demonstrate with unittest as it is included directly in Python. The key to this test is to call plugin.test_object_serialization() with an instance of your dataclass that you want to test: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): self . assertTrue ( plugin . test_object_serialization ( example_plugin . PodScenarioResults ( [ example_plugin . Pod ( namespace = \"default\" , name = \"nginx-asdf\" ) ] ) )) Remember, you need to call this function with an instance containing actual data, not just the class name. The test function will first serialize, then unserialize your data and check if it\u2019s the same. If you want to use a manually created schema, you can do so, too: class ExamplePluginTest ( unittest . TestCase ): def test_serialization ( self ): plugin . test_object_serialization ( example_plugin . PodScenarioResults ( #... ), schema . ObjectType ( #... ) )","title":"Writing a serialization test"},{"location":"arcaflow/creating-plugins/python/#functional-tests","text":"Functional tests don\u2019t have anything special about them. You can directly call your code with your dataclasses as parameters, and check the return. This works best on auto-generated schemas with the @plugin.step decorator. See below for manually created schemas. class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): input = example_plugin . PodScenarioParams () output_id , output_data = example_plugin . pod_scenario ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) ) If you created your schema manually, the best way to write your tests is to include the schema in your test. This will automatically validate both the input and the output, making sure they conform to your schema. For example: class ExamplePluginTest ( unittest . TestCase ): def test_functional ( self ): step_schema = schema . StepSchema ( #... handler = example_plugin . pod_scenario , ) input = example_plugin . PodScenarioParams () output_id , output_data = step_schema ( input ) # Check if the output is always an error, as it is the case for the example plugin. self . assertEqual ( \"error\" , output_id ) self . assertEqual ( output_data , example_plugin . PodScenarioError ( \"Cannot kill pod .* in namespace .*, function not implemented\" ) )","title":"Functional tests"},{"location":"arcaflow/creating-plugins/python/#embedding-your-plugin","text":"Instead of using your plugin as a standalone tool or in conjunction with Arcaflow, you can also embed your plugin into your existing Python application. To do that you simply build a schema using one of the methods described above and then call the schema yourself. You can pass raw data as an input, and you\u2019ll get the benefit of schema validation. # Build your schema using the schema builder from above with the step functions passed. schema = plugin . build_schema ( pod_scenario ) # Which step we want to execute step_id = \"pod\" # Input parameters. Note, these must be a dict, not a dataclass step_params = { \"pod_name_pattern\" : \".*\" , \"pod_namespace_pattern\" : \".*\" , } # Execute the step output_id , output_data = schema ( step_id , step_params ) # Print which kind of result we have pprint . pprint ( output_id ) # Print the result data pprint . pprint ( output_data ) However, the example above requires you to provide the data as a dict , not a dataclass , and it will also return a dict as an output object. Sometimes, you may want to use a partial approach, where you only use part of the SDK. In this case, you can change your code to run any of the following functions, in order: serialization.load_from_file() to load a YAML or JSON file into a dict yourschema.unserialize_input() to turn a dict into a dataclass needed for your steps yourschema.call_step() to run a step with the unserialized dataclass yourschema.serialize_output() to turn the output dataclass into a dict","title":"Embedding your plugin"},{"location":"arcaflow/creating-plugins/python/#faq","text":"","title":"FAQ"},{"location":"arcaflow/creating-plugins/python/#how-can-i-add-a-field-with-dashes-such-as-my-field","text":"Dataclasses don\u2019t support dashes in parameters. You can work around this by defining the id metadata field: @dataclasses . dataclass class MyData : my_field : str = dataclasses . field ( metadata = { \"id\" : \"my-field\" })","title":"How can I add a field with dashes, such as my-field?"},{"location":"arcaflow/creating-plugins/python/#how-can-i-write-a-dataclass-from-a-schema-to-a-yaml-or-json-file","text":"You can extend Pythons JSON encoder to support dataclasses. If that doesn\u2019t suit your needs, you can use this SDK to convert the dataclasses to their basic representations and then write that to your JSON or YAML file. First, add this outside of your step: my_object_schema = plugin . build_object_schema ( YourDataclass ) Inside your step function you can then dump the data from your input def your_step ( params : YourParams ) yaml_contents = yaml . dump ( my_object_schema . serialize ( params . some_param ))","title":"How can I write a dataclass from a schema to a YAML or JSON file?"},{"location":"arcaflow/creating-plugins/python/#how-can-i-easily-load-a-list-from-a-yaml-or-json-into-a-list-of-dataclasses","text":"This requires a bit of trickery. First, we build a schema from the dataclass representing the row or entry in the list: my_row_schema = plugin . build_object_schema ( MyRow ) Then you can create a list schema: my_list_schema = schema . ListType ( my_row_schema ) You can now unserialize a list obtained from the YAML or JSON file: my_data = my_list_schema . unserialize ( json . loads ( ... ))","title":"How can I easily load a list from a YAML or JSON into a list of dataclasses?"},{"location":"arcalog/","text":"","title":"Arcalog"}]}